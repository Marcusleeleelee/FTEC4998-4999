{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcusleeleelee/FTEC4998-4999/blob/main/FTEC4998_4999.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Irism69Bo6L",
        "outputId": "9212e8d2-ef2c-4bb1-e488-5f0a6684791b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn models and utilities\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Scikit-learn utilities\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# PyTorch for neural network models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Progress bar for loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Plotting accuracy curves and other metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Others\n",
        "\n",
        "# Google Colab specific (if needed)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lmLx5bbgBtcq"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, file_path):\n",
        "        self.dataset = pd.read_feather(file_path)\n",
        "        self.original = self.dataset.copy()\n",
        "        self.X_train, self.y_train = None, None\n",
        "        self.X_valid, self.y_valid = None, None\n",
        "        self.X_test, self.y_test = None, None\n",
        "        self.label = 'loan_condition_cat'\n",
        "        self.min_max_columns = ['annual_inc', 'year']\n",
        "        self.means = {}\n",
        "        self.stds = {}\n",
        "        self.mins = {}\n",
        "        self.maxs = {}\n",
        "\n",
        "    def show(self, rows=10):\n",
        "        return self.dataset.head(rows)\n",
        "\n",
        "    def basic_processing(self):\n",
        "        temp_func_2 = lambda x: {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}[str(x)]\n",
        "        columns_to_delete = [\n",
        "            'id', 'issue_d', 'home_ownership_cat', 'income_category', 'income_cat', 'term_cat',\n",
        "            'application_type_cat', 'purpose_cat', 'interest_payment_cat', 'loan_condition'\n",
        "        ]\n",
        "        self.dataset.drop(columns=columns_to_delete, inplace=True)\n",
        "        self.dataset['grade'] = self.dataset['grade'].apply(temp_func_2)\n",
        "        self.dataset['final_d'] = self.dataset['final_d'].apply(lambda x: str(x)[-4:]).apply(int)\n",
        "        self.dataset['year'] = self.dataset['year'].apply(lambda x: str(x)[-4:]).apply(int)\n",
        "        self.dataset = pd.get_dummies(self.dataset, columns=['home_ownership', 'term', 'application_type',\n",
        "                                                             'purpose', 'interest_payments', 'region'], dtype=int)\n",
        "\n",
        "    def train_test_split(self, test_size=0.2, valid_size=0.2, random_state=42):\n",
        "        # Split into train and test first\n",
        "        X = self.dataset.drop(columns=[self.label])\n",
        "        y = self.dataset[self.label]\n",
        "        X_train_full, self.X_test, y_train_full, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "\n",
        "        # Further split training data into training and validation sets\n",
        "        self.X_train, self.X_valid, self.y_train, self.y_valid = train_test_split(X_train_full, y_train_full, test_size=valid_size, random_state=random_state)\n",
        "\n",
        "        # Save original columns for reference\n",
        "        self.original_columns = X.columns\n",
        "\n",
        "        # Sort and reset index for consistency\n",
        "        for dataset in [self.X_train, self.X_valid, self.X_test, self.y_train, self.y_valid, self.y_test]:\n",
        "            dataset.sort_index(inplace=True)\n",
        "            dataset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def preprocessing_train(self):\n",
        "        # Separate columns for Min-Max and Z-score normalization\n",
        "        columns_to_normalize = self.min_max_columns\n",
        "        columns_to_scale = [col for col in self.X_train.columns if col not in columns_to_normalize]\n",
        "\n",
        "        # Z-score normalization\n",
        "        for col in columns_to_scale:\n",
        "            mean = np.mean(self.X_train[col])\n",
        "            std = np.std(self.X_train[col])\n",
        "            self.means[col] = mean\n",
        "            self.stds[col] = std\n",
        "            self.X_train[col] = (self.X_train[col] - mean) / std\n",
        "\n",
        "        # Min-Max normalization\n",
        "        for col in columns_to_normalize:\n",
        "            min_val = np.min(self.X_train[col])\n",
        "            max_val = np.max(self.X_train[col])\n",
        "            self.mins[col] = min_val\n",
        "            self.maxs[col] = max_val\n",
        "            self.X_train[col] = (self.X_train[col] - min_val) / (max_val - min_val)\n",
        "\n",
        "        # Perform PCA\n",
        "        selected_columns = self.perform_pca(self.X_train, n_components=35)\n",
        "        self.X_train = self.X_train[selected_columns]\n",
        "\n",
        "    def perform_pca(self, data, n_components):\n",
        "        # Center the data\n",
        "        data_mean = np.mean(data, axis=0)\n",
        "        centered_data = data - data_mean\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov_matrix = np.cov(centered_data, rowvar=False)\n",
        "\n",
        "        # Eigen decomposition\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "        # Sort eigenvectors by eigenvalues in descending order\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "        # Select the top n_components\n",
        "        selected_eigenvectors = sorted_eigenvectors[:, :n_components]\n",
        "\n",
        "        # Identify important features\n",
        "        feature_importance = np.abs(selected_eigenvectors).sum(axis=1)\n",
        "        important_indices = np.argsort(feature_importance)[::-1][:n_components]\n",
        "\n",
        "        # Return the original column names of these features\n",
        "        important_features = [self.original_columns[i] for i in important_indices]\n",
        "\n",
        "        return important_features\n",
        "\n",
        "    def preprocessing_test(self):\n",
        "        # Separate columns for Min-Max and Z-score normalization\n",
        "        columns_to_normalize = self.min_max_columns\n",
        "        columns_to_scale = [col for col in self.X_test.columns if col not in columns_to_normalize]\n",
        "\n",
        "        # Apply Z-score normalization using training statistics\n",
        "        for col in columns_to_scale:\n",
        "            self.X_test[col] = (self.X_test[col] - self.means[col]) / self.stds[col]\n",
        "\n",
        "        # Apply Min-Max normalization using training statistics\n",
        "        for col in columns_to_normalize:\n",
        "            self.X_test[col] = (self.X_test[col] - self.mins[col]) / (self.maxs[col] - self.mins[col])\n",
        "\n",
        "        # Apply PCA using training components\n",
        "        self.X_test = self.X_test[[i for i in self.X_train.columns.to_list()]]\n",
        "\n",
        "    def preprocessing_valid(self):\n",
        "        # Separate columns for Min-Max and Z-score normalization\n",
        "        columns_to_normalize = self.min_max_columns\n",
        "        columns_to_scale = [col for col in self.X_valid.columns if col not in columns_to_normalize]\n",
        "\n",
        "        # Apply Z-score normalization using training statistics\n",
        "        for col in columns_to_scale:\n",
        "            self.X_valid[col] = (self.X_valid[col] - self.means[col]) / self.stds[col]\n",
        "\n",
        "        # Apply Min-Max normalization using training statistics\n",
        "        for col in columns_to_normalize:\n",
        "            self.X_valid[col] = (self.X_valid[col] - self.mins[col]) / (self.maxs[col] - self.mins[col])\n",
        "\n",
        "        # Apply PCA using training components\n",
        "        self.X_valid = self.X_valid[[i for i in self.X_train.columns.to_list()]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "13kLSUwPBvO9"
      },
      "outputs": [],
      "source": [
        "# Calculating # ok\n",
        "data = Dataset('/content/drive/My Drive/Colab Notebooks/FTEC4998_9/loan_final313_processed.feather')\n",
        "data.basic_processing()\n",
        "data.train_test_split(test_size=0.15, valid_size=0.15)\n",
        "data.preprocessing_train()\n",
        "data.preprocessing_valid()\n",
        "data.preprocessing_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMPCAhUjB4tJ",
        "outputId": "b09af39e-67d9-4200-dd4f-9dc23c9cf8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.61% of the training set belongs to the positive class.\n",
            "Training set shape: (641131, 35), (641131,)\n",
            "Validation set shape: (113141, 35), (113141,)\n",
            "Test set shape: (133107, 35), (133107,)\n",
            "Training type: <class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.series.Series'>\n",
            "Validation type: <class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.series.Series'>\n",
            "Test type: <class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# Data conversion\n",
        "train_x, train_y = data.X_train, data.y_train\n",
        "valid_x, valid_y = data.X_valid, data.y_valid\n",
        "test_x, test_y = data.X_test, data.y_test\n",
        "\n",
        "# Print the percentage of positive class in the training set\n",
        "counts = np.mean(train_y == 1) * 100\n",
        "print(f\"{counts:.2f}% of the training set belongs to the positive class.\")\n",
        "\n",
        "# Print the shapes of the datasets\n",
        "print(f\"Training set shape: {train_x.shape}, {train_y.shape}\")\n",
        "print(f\"Validation set shape: {valid_x.shape}, {valid_y.shape}\")\n",
        "print(f\"Test set shape: {test_x.shape}, {test_y.shape}\")\n",
        "\n",
        "# Print the types of the datasets\n",
        "print(f\"Training type: {type(train_x)}, {type(train_y)}\")\n",
        "print(f\"Validation type: {type(valid_x)}, {type(valid_y)}\")\n",
        "print(f\"Test type: {type(test_x)}, {type(test_y)}\")\n",
        "\n",
        "# Ensure y_train, y_valid, and y_test are binary\n",
        "assert set(train_y).issubset({0, 1}), \"Target values for train_y must be 0 or 1 for binary classification.\"\n",
        "assert set(valid_y).issubset({0, 1}), \"Target values for valid_y must be 0 or 1 for binary classification.\"\n",
        "assert set(test_y).issubset({0, 1}), \"Target values for test_y must be 0 or 1 for binary classification.\"\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "5UL36a0-Tq97"
      },
      "outputs": [],
      "source": [
        "# Train, predict, and accuracy functions\n",
        "# Function to train the model and store the loss trajectory\n",
        "def train_model_with_loss_tracking(model, train_x, train_y):\n",
        "    \"\"\"\n",
        "    Train the model, track the loss at each epoch, and return the loss history.\n",
        "    \"\"\"\n",
        "    loss_history = []\n",
        "\n",
        "    # Loop over the number of epochs\n",
        "    for epoch in range(model.epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(train_x)\n",
        "        loss = model.criterion(output, train_y)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        model.optimizer.zero_grad()  # Zero the gradients\n",
        "        loss.backward()  # Backpropagate the loss\n",
        "        model.optimizer.step()  # Update the weights\n",
        "\n",
        "        # Save the loss for this epoch\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        # Print loss every 50 epochs for tracking\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            print(f'Epoch [{epoch + 1}/{model.epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "def predict_model_pt(model, X): # ok\n",
        "    if isinstance(X, pd.DataFrame): X = torch.tensor(X.to_numpy(), dtype=torch.float32)  # Convert DataFrame to tensor\n",
        "    X = X.to(next(model.parameters()).device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X).squeeze()\n",
        "        return (outputs > 0.5).float().cpu().numpy()  # Convert to numpy array for output\n",
        "\n",
        "def calculate_accuracy_pt(model, X, y, pred=None): # 95% ok\n",
        "    # Ensure X and y are on the same device as the model\n",
        "\n",
        "    if str(type(X)) != \"<class 'torch.Tensor'>\": X, y = df_to_tensor(X, y)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = torch.tensor(predict_model_pt(model, X), dtype=torch.float32).to(device) if pred is None else torch.tensor(pred, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Ensure predictions and labels are the same shape\n",
        "    predictions = predictions.squeeze()\n",
        "    y = y.squeeze()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct = (predictions == y).sum().item()\n",
        "    accuracy = correct / len(y)\n",
        "    return accuracy\n",
        "\n",
        "# Function to calculate precision, recall, and AUC\n",
        "def evaluate_metrics(model, X, y):\n",
        "    # Evaluate precision, recall, and AUC for a given model and dataset.\n",
        "    # Get the predicted probabilities (before the 0.5 threshold)\n",
        "    X, y = df_to_tensor(X, y)\n",
        "    y_proba = model(X).cpu().detach().numpy()\n",
        "\n",
        "    # Get binary predictions using the threshold of 0.5\n",
        "    y_pred = (y_proba > 0.5).astype(float)\n",
        "    y_true = y.cpu().numpy()\n",
        "\n",
        "    # Calculate precision, recall, and AUC\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    auc = roc_auc_score(y_true, y_proba)\n",
        "\n",
        "    return precision, recall, auc\n",
        "\n",
        "def df_to_tensor(x, y, device=device): # ok\n",
        "    assert isinstance(x, pd.DataFrame) and isinstance(y, pd.Series), str((type(x), type(y)))\n",
        "    return (torch.tensor(x.to_numpy(), dtype=torch.float32).to(device),  torch.tensor(y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports for precision, recall, AUC, and plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the Logistic Regression model as a simple neural network\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, train_x, train_y, lr=0.05):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "\n",
        "        # Convert training data to tensors and move to the specified device (GPU/CPU)\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "\n",
        "        # Set input dimension based on the number of features in train_x\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "\n",
        "        # Define the neural network architecture: One Linear layer followed by a Sigmoid activation\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 1),  # Linear layer mapping input to one output\n",
        "            nn.Sigmoid()  # Sigmoid activation for binary classification\n",
        "        )\n",
        "\n",
        "        # Define the loss function (Binary Cross-Entropy) and the optimizer (Adam)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        # Set the number of training epochs\n",
        "        self.epochs = 600\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: Compute the output of the network given input x.\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "# Instantiate the Logistic Regression model\n",
        "lrg_model = LogisticRegressionModel(train_x, train_y)\n",
        "\n",
        "# Move the model to the specified device (GPU or CPU)\n",
        "lrg_model.to(device)\n",
        "\n",
        "# Train the model and track loss trajectory\n",
        "loss_history = train_model_with_loss_tracking(lrg_model, lrg_model.train_x, lrg_model.train_y)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "pred_valid = predict_model_pt(lrg_model, valid_x)\n",
        "\n",
        "# Calculate validation accuracy\n",
        "accuracy_valid = calculate_accuracy_pt(lrg_model, valid_x, valid_y, pred_valid)\n",
        "\n",
        "# Calculate precision, recall, and AUC for the validation set\n",
        "precision_valid, recall_valid, auc_valid = evaluate_metrics(lrg_model, valid_x, valid_y)\n",
        "\n",
        "# Print validation metrics\n",
        "print('Validation Accuracy:', round(accuracy_valid, 5))\n",
        "print('Validation Precision:', round(precision_valid, 5))\n",
        "print('Validation Recall:', round(recall_valid, 5))\n",
        "print('Validation AUC:', round(auc_valid, 5))\n",
        "\n",
        "# Plot the loss trajectory\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, lrg_model.epochs + 1), loss_history, linestyle='-')\n",
        "plt.title('Loss Trajectory during Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7t3X87q1yqJB",
        "outputId": "6a9ad661-27f2-4f6b-b659-8a8b42f91754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/600], Loss: 0.1977\n",
            "Epoch [100/600], Loss: 0.1689\n",
            "Epoch [150/600], Loss: 0.1634\n",
            "Epoch [200/600], Loss: 0.1614\n",
            "Epoch [250/600], Loss: 0.1605\n",
            "Epoch [300/600], Loss: 0.1601\n",
            "Epoch [350/600], Loss: 0.1598\n",
            "Epoch [400/600], Loss: 0.1596\n",
            "Epoch [450/600], Loss: 0.1595\n",
            "Epoch [500/600], Loss: 0.1594\n",
            "Epoch [550/600], Loss: 0.1593\n",
            "Epoch [600/600], Loss: 0.1592\n",
            "Validation Accuracy: 0.94936\n",
            "Validation Precision: 0.8713\n",
            "Validation Recall: 0.38394\n",
            "Validation AUC: 0.89446\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the MLP model for binary classification\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, train_x, train_y, lr=0.05):\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        # Convert training data to tensors and move to the specified device (GPU/CPU)\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "\n",
        "        # Set input dimension based on the number of features in train_x\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "\n",
        "        # Define the MLP architecture\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Define the loss function (Binary Cross-Entropy) and the optimizer (Adam)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "\n",
        "        # Set the number of training epochs\n",
        "        self.epochs = 600\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass: Compute the output of the network given input x.\n",
        "        \"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "    def plot_model_architecture(self):\n",
        "        \"\"\"\n",
        "        Plots the architecture of the PyTorch model by visualizing the layers.\n",
        "\n",
        "        Args:\n",
        "        - model: A PyTorch model (should be nn.Sequential or contain a sequential-like structure).\n",
        "        \"\"\"\n",
        "        # Extract layers from the model (assuming it's a nn.Sequential model)\n",
        "        layers = []\n",
        "        for layer in self.model.net:\n",
        "            layers.append(str(layer))\n",
        "\n",
        "        # Define the number of layers and nodes in each layer\n",
        "        num_layers = len(layers)\n",
        "\n",
        "        # Create a figure\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.title('MLP Model Architecture', fontsize=16)\n",
        "\n",
        "        # Plotting the layers\n",
        "        y_offset = 0\n",
        "        for i, layer in enumerate(layers):\n",
        "            layer_name = layer.split('(')[0]  # Get the layer name (Linear, ReLU, etc.)\n",
        "\n",
        "            # Plot the layer type as text\n",
        "            plt.text(0.5, y_offset, layer_name, fontsize=12, ha='center')\n",
        "\n",
        "            # Draw a rectangle for each layer to represent it visually\n",
        "            plt.gca().add_patch(plt.Rectangle((0.3, y_offset - 0.2), 0.4, 0.4, fill=True, edgecolor='black', facecolor='lightgray'))\n",
        "\n",
        "            # Update y_offset for the next layer\n",
        "            y_offset -= 1.0\n",
        "\n",
        "        # Adjust plot limits and hide axes\n",
        "        plt.gca().set_xlim(0, 1)\n",
        "        plt.gca().set_ylim(y_offset + 0.5, 0.5)\n",
        "        plt.gca().axis('off')\n",
        "\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "\n",
        "# Instantiate the MLP model\n",
        "mlp_model = MLPModel(train_x, train_y)\n",
        "\n",
        "# Move the model to the specified device (GPU or CPU)\n",
        "mlp_model.to(device)\n",
        "\n",
        "# Train the model and track loss trajectory\n",
        "loss_history = train_model_with_loss_tracking(mlp_model, mlp_model.train_x, mlp_model.train_y)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "pred_valid = predict_model_pt(mlp_model, valid_x)\n",
        "\n",
        "# Calculate validation accuracy\n",
        "accuracy_valid = calculate_accuracy_pt(mlp_model, valid_x, valid_y, pred_valid)\n",
        "\n",
        "# Calculate precision, recall, and AUC for the validation set\n",
        "precision_valid, recall_valid, auc_valid = evaluate_metrics(mlp_model, valid_x, valid_y)\n",
        "\n",
        "# Print validation metrics\n",
        "print('Validation Accuracy:', round(accuracy_valid, 5))\n",
        "print('Validation Precision:', round(precision_valid, 5))\n",
        "print('Validation Recall:', round(recall_valid, 5))\n",
        "print('Validation AUC:', round(auc_valid, 5))\n",
        "\n",
        "# Plot the loss trajectory\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, mlp_model.epochs + 1), loss_history, linestyle='-')\n",
        "plt.title('Loss Trajectory during Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot model architecture\n",
        "mlp_model.plot_model_architecture()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "tBjeVS2cTyWh",
        "outputId": "78ee0f7b-5563-45f3-e098-3ef3056667fc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/600], Loss: 0.1447\n",
            "Epoch [100/600], Loss: 0.1252\n",
            "Epoch [150/600], Loss: 0.1116\n",
            "Epoch [200/600], Loss: 0.1069\n",
            "Epoch [250/600], Loss: 0.1058\n",
            "Epoch [300/600], Loss: 0.1065\n",
            "Epoch [350/600], Loss: 0.1067\n",
            "Epoch [400/600], Loss: 0.1052\n",
            "Epoch [450/600], Loss: 0.1036\n",
            "Epoch [500/600], Loss: 0.1045\n",
            "Epoch [550/600], Loss: 0.1031\n",
            "Epoch [600/600], Loss: 0.1039\n",
            "Validation Accuracy: 0.9607\n",
            "Validation Precision: 0.94805\n",
            "Validation Recall: 0.50569\n",
            "Validation AUC: 0.9592\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression as a neural network\n",
        "class LogisticRegressionModel(nn.Module): # Not yet finish the plot function\n",
        "    def __init__(self, train_x, train_y, lr=0.001):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 600\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "lrg = LogisticRegressionModel(train_x, train_y)\n",
        "lrg.to(device)\n",
        "ABC = train_model_pt(lrg)\n",
        "pred_train = predict_model_pt(lrg, train_x)\n",
        "accuracy_train = calculate_accuracy_pt(lrg, train_x, train_y, pred_train)\n",
        "pred_valid = predict_model_pt(lrg, valid_x)\n",
        "accuracy_valid = calculate_accuracy_pt(lrg, valid_x, valid_y, pred_valid)\n",
        "pred_test = predict_model_pt(lrg, test_x)\n",
        "accuracy_test = calculate_accuracy_pt(lrg, test_x, test_y, pred_test)\n",
        "print('Train pred:', pred_train)\n",
        "print('Train acc:', accuracy_train)\n",
        "print('Valid pred:', pred_valid)\n",
        "print('Valid acc:', accuracy_valid)\n",
        "print('Test pred:', pred_test)\n",
        "print('Test acc:', accuracy_test)\n",
        "# Extract epochs and accuracies\n",
        "epochs = list(ABC.keys())\n",
        "accuracies = list(ABC.values())\n",
        "\n",
        "# Plotting accuracy over epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, accuracies, marker='o', linestyle='-')\n",
        "plt.title('Training Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        },
        "id": "_Wxx6iHbm_4u",
        "outputId": "df2d7b91-aa35-4ef0-f4b3-0394e90cb041"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.9161598682403564\n",
            "Epoch 50, Loss: 0.8374332785606384\n",
            "Epoch 100, Loss: 0.772408127784729\n",
            "Epoch 150, Loss: 0.7173776030540466\n",
            "Epoch 200, Loss: 0.6708447933197021\n",
            "Epoch 250, Loss: 0.6318569183349609\n",
            "Epoch 300, Loss: 0.5991731286048889\n",
            "Epoch 350, Loss: 0.5716618895530701\n",
            "Epoch 400, Loss: 0.5482798218727112\n",
            "Epoch 450, Loss: 0.5279086232185364\n",
            "Epoch 500, Loss: 0.5097042322158813\n",
            "Epoch 550, Loss: 0.4931057393550873\n",
            "Train pred: [1. 0. 0. ... 0. 0. 0.]\n",
            "Train acc: 0.9167440039555099\n",
            "Valid pred: [1. 0. 1. ... 0. 0. 0.]\n",
            "Valid acc: 0.9171211143617256\n",
            "Test pred: [0. 0. 1. ... 0. 0. 0.]\n",
            "Test acc: 0.9165483408085224\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Oblivious Decision Tree model\n",
        "class ObliviousDecisionTree(nn.Module):\n",
        "    def __init__(self, train_x, train_y, depth=6, lr=0.01, epochs=50, optimizer_type='adam', loss_fn='mse', lr_scheduler=None):\n",
        "        super(ObliviousDecisionTree, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.depth = depth\n",
        "\n",
        "        # Initialize feature selectors and thresholds for each depth level\n",
        "        self.feature_selectors = nn.ParameterList(\n",
        "            [nn.Parameter(torch.randn(self.input_dim)) for _ in range(depth)]\n",
        "        )\n",
        "        self.thresholds = nn.ParameterList(\n",
        "            [nn.Parameter(torch.randn(1)) for _ in range(depth)]\n",
        "        )\n",
        "\n",
        "        # Initialize leaf values (2^depth leaves)\n",
        "        self.leaf_values = nn.Parameter(torch.randn(2 ** depth))\n",
        "\n",
        "        # Loss Function\n",
        "        if loss_fn == 'mse': self.criterion = nn.MSELoss()\n",
        "        elif loss_fn == 'bce': self.criterion = nn.BCELoss()\n",
        "        else: raise ValueError(\"Unsupported loss function. Use 'mse' or 'bce'.\")\n",
        "\n",
        "        # Optimizer\n",
        "        if optimizer_type == 'adam': self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        elif optimizer_type == 'sgd': self.optimizer = optim.SGD(self.parameters(), lr=lr, momentum=0.9)\n",
        "        else: raise ValueError(\"Unsupported optimizer type. Use 'adam' or 'sgd'.\")\n",
        "\n",
        "        # Learning Rate Scheduler (optional)\n",
        "        self.lr_scheduler = None\n",
        "        if lr_scheduler == 'step': self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.1)\n",
        "        elif lr_scheduler == 'plateau': self.lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
        "\n",
        "        # Number of training epochs\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        node_indices = torch.zeros(batch_size, dtype=torch.long).to(x.device)\n",
        "\n",
        "        for d in range(self.depth):\n",
        "            feature_selector = self.feature_selectors[d]\n",
        "            threshold = self.thresholds[d]\n",
        "            feature_value = torch.matmul(x, feature_selector)\n",
        "            go_right = feature_value > threshold\n",
        "            node_indices = (node_indices << 1) | go_right.long()\n",
        "        return self.leaf_values[node_indices].unsqueeze(1)\n",
        "odt = ObliviousDecisionTree(train_x, train_y, depth=3, lr=0.01, epochs=500, optimizer_type='adam', loss_fn='mse', lr_scheduler='step')\n",
        "odt.to(device)\n",
        "train_model_pt(odt)\n",
        "pred_train = predict_model_pt(odt, train_x)\n",
        "accuracy_train = calculate_accuracy_pt(odt, train_x, train_y, pred_train)\n",
        "pred_valid = predict_model_pt(odt, valid_x)\n",
        "accuracy_valid = calculate_accuracy_pt(odt, valid_x, valid_y, pred_valid)\n",
        "pred_test = predict_model_pt(odt, test_x)\n",
        "accuracy_test = calculate_accuracy_pt(odt, test_x, test_y, pred_test)\n",
        "print('Train pred:', pred_train)\n",
        "print('Train acc:', accuracy_train)\n",
        "print('Valid pred:', pred_valid)\n",
        "print('Valid acc:', accuracy_valid)\n",
        "print('Test pred:', pred_test)\n",
        "print('Test acc:', accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-xTf6Xyr9kq",
        "outputId": "3de4b170-fab4-4053-d357-143420e7a66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.141756296157837\n",
            "Epoch 5, Loss: 1.0622385740280151\n",
            "Epoch 10, Loss: 0.9878339171409607\n",
            "Epoch 15, Loss: 0.9178289175033569\n",
            "Epoch 20, Loss: 0.8519766330718994\n",
            "Epoch 25, Loss: 0.790435254573822\n",
            "Epoch 30, Loss: 0.7329530119895935\n",
            "Epoch 35, Loss: 0.6791958212852478\n",
            "Epoch 40, Loss: 0.6289980411529541\n",
            "Epoch 45, Loss: 0.582176923751831\n",
            "Epoch 50, Loss: 0.5385658740997314\n",
            "Epoch 55, Loss: 0.49804726243019104\n",
            "Epoch 60, Loss: 0.4604746401309967\n",
            "Epoch 65, Loss: 0.42569631338119507\n",
            "Epoch 70, Loss: 0.39356255531311035\n",
            "Epoch 75, Loss: 0.3639194965362549\n",
            "Epoch 80, Loss: 0.33662155270576477\n",
            "Epoch 85, Loss: 0.31152424216270447\n",
            "Epoch 90, Loss: 0.2884862422943115\n",
            "Epoch 95, Loss: 0.2673698961734772\n",
            "Epoch 100, Loss: 0.24804306030273438\n",
            "Epoch 105, Loss: 0.23037898540496826\n",
            "Epoch 110, Loss: 0.21425725519657135\n",
            "Epoch 115, Loss: 0.1995640993118286\n",
            "Epoch 120, Loss: 0.18619200587272644\n",
            "Epoch 125, Loss: 0.1740388572216034\n",
            "Epoch 130, Loss: 0.16300968825817108\n",
            "Epoch 135, Loss: 0.15301498770713806\n",
            "Epoch 140, Loss: 0.14396972954273224\n",
            "Epoch 145, Loss: 0.135796457529068\n",
            "Epoch 150, Loss: 0.12842290103435516\n",
            "Epoch 155, Loss: 0.12178028374910355\n",
            "Epoch 160, Loss: 0.11580504477024078\n",
            "Epoch 165, Loss: 0.11043864488601685\n",
            "Epoch 170, Loss: 0.1056271344423294\n",
            "Epoch 175, Loss: 0.10131989419460297\n",
            "Epoch 180, Loss: 0.0974707305431366\n",
            "Epoch 185, Loss: 0.09403662383556366\n",
            "Epoch 190, Loss: 0.09097836166620255\n",
            "Epoch 195, Loss: 0.08825951814651489\n",
            "Epoch 200, Loss: 0.08584652841091156\n",
            "Epoch 205, Loss: 0.08370906859636307\n",
            "Epoch 210, Loss: 0.08181902021169662\n",
            "Epoch 215, Loss: 0.08015081286430359\n",
            "Epoch 220, Loss: 0.07868100702762604\n",
            "Epoch 225, Loss: 0.07738842815160751\n",
            "Epoch 230, Loss: 0.07625390589237213\n",
            "Epoch 235, Loss: 0.07525987178087234\n",
            "Epoch 240, Loss: 0.07439044862985611\n",
            "Epoch 245, Loss: 0.07363138347864151\n",
            "Epoch 250, Loss: 0.0729699656367302\n",
            "Epoch 255, Loss: 0.07239465415477753\n",
            "Epoch 260, Loss: 0.07189511507749557\n",
            "Epoch 265, Loss: 0.07146214693784714\n",
            "Epoch 270, Loss: 0.0710875391960144\n",
            "Epoch 275, Loss: 0.07076402008533478\n",
            "Epoch 280, Loss: 0.07048508524894714\n",
            "Epoch 285, Loss: 0.07024502754211426\n",
            "Epoch 290, Loss: 0.07003875821828842\n",
            "Epoch 295, Loss: 0.06986184418201447\n",
            "Epoch 300, Loss: 0.06971035897731781\n",
            "Epoch 305, Loss: 0.06958087533712387\n",
            "Epoch 310, Loss: 0.0694703757762909\n",
            "Epoch 315, Loss: 0.06937626004219055\n",
            "Epoch 320, Loss: 0.0692962110042572\n",
            "Epoch 325, Loss: 0.06922823190689087\n",
            "Epoch 330, Loss: 0.06917061656713486\n",
            "Epoch 335, Loss: 0.06912185996770859\n",
            "Epoch 340, Loss: 0.0690806582570076\n",
            "Epoch 345, Loss: 0.06904590874910355\n",
            "Epoch 350, Loss: 0.06901663541793823\n",
            "Epoch 355, Loss: 0.068992018699646\n",
            "Epoch 360, Loss: 0.06897137314081192\n",
            "Epoch 365, Loss: 0.06895405799150467\n",
            "Epoch 370, Loss: 0.06893956661224365\n",
            "Epoch 375, Loss: 0.06892746686935425\n",
            "Epoch 380, Loss: 0.06891737133264542\n",
            "Epoch 385, Loss: 0.06890895962715149\n",
            "Epoch 390, Loss: 0.06890197098255157\n",
            "Epoch 395, Loss: 0.06889616698026657\n",
            "Epoch 400, Loss: 0.06889136880636215\n",
            "Epoch 405, Loss: 0.0688873901963234\n",
            "Epoch 410, Loss: 0.06888409703969955\n",
            "Epoch 415, Loss: 0.06888138502836227\n",
            "Epoch 420, Loss: 0.0688791424036026\n",
            "Epoch 425, Loss: 0.06887730211019516\n",
            "Epoch 430, Loss: 0.06887580454349518\n",
            "Epoch 435, Loss: 0.06887456029653549\n",
            "Epoch 440, Loss: 0.06887355446815491\n",
            "Epoch 445, Loss: 0.06887273490428925\n",
            "Epoch 450, Loss: 0.06887207180261612\n",
            "Epoch 455, Loss: 0.06887151300907135\n",
            "Epoch 460, Loss: 0.06887107342481613\n",
            "Epoch 465, Loss: 0.06887070834636688\n",
            "Epoch 470, Loss: 0.0688704177737236\n",
            "Epoch 475, Loss: 0.0688701793551445\n",
            "Epoch 480, Loss: 0.06886999309062958\n",
            "Epoch 485, Loss: 0.06886983662843704\n",
            "Epoch 490, Loss: 0.0688697099685669\n",
            "Epoch 495, Loss: 0.06886962056159973\n",
            "Train pred: [0. 0. 0. ... 0. 0. 0.]\n",
            "Train acc: 0.923898548034645\n",
            "Valid pred: [0. 0. 0. ... 0. 0. 0.]\n",
            "Valid acc: 0.9247222492288384\n",
            "Test pred: [0. 0. 0. ... 0. 0. 0.]\n",
            "Test acc: 0.9239634279188923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "class SVMClassifier():\n",
        "    def __init__(self, train_x, train_y, fraction=0.1, C=0.1, kernel='poly', degree=3, gamma='scale', n_estimators=6, max_samples=0.001, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the SVMClassifier with training data and optional parameters.\n",
        "\n",
        "        Parameters:\n",
        "        - train_x: Features for training.\n",
        "        - train_y: Labels for training.\n",
        "        - fraction: Fraction of data to sample (not used in this implementation).\n",
        "        - C: Penalty parameter C of the error term for SVM.\n",
        "        - kernel: Kernel type to be used in the SVM algorithm.\n",
        "        - degree: Degree of the polynomial kernel function (if kernel='poly').\n",
        "        - gamma: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.\n",
        "        - n_estimators: The number of base estimators in the Bagging ensemble.\n",
        "        - max_samples: The number of samples to draw from X to train each base estimator.\n",
        "        - random_state: Controls the randomness of the bootstrapping of the samples.\n",
        "        \"\"\"\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.C = C\n",
        "        self.kernel = kernel\n",
        "        self.degree = degree\n",
        "        self.gamma = gamma\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.model = None\n",
        "        self.accuracies = []\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Fits the SVM model using Bagging with the specified parameters.\"\"\"\n",
        "        self.model = BaggingClassifier(\n",
        "            estimator=SVC(C=self.C, kernel=self.kernel, degree=self.degree, gamma=self.gamma),\n",
        "            n_estimators=self.n_estimators,\n",
        "            max_samples=self.max_samples,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "        self._update_accuracy()\n",
        "\n",
        "    def _update_accuracy(self):\n",
        "        \"\"\"Internal method to update and store the model's accuracy on the training set.\"\"\"\n",
        "        predictions = self.model.predict(self.train_x)\n",
        "        accuracy = accuracy_score(self.train_y, predictions)\n",
        "        self.accuracies.append(accuracy)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts the labels for the given input data X.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call `fit` before `predict`.\")\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        \"\"\"Calculates the accuracy of the model on the given data X and true labels y.\"\"\"\n",
        "        if pred is None: pred = self.predict(X)\n",
        "        return accuracy_score(y, pred)\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        \"\"\"Plots the training accuracy curve after fitting the model.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o', linestyle='-')\n",
        "        plt.title('SVM Training Accuracy Curve')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "svm = SVMClassifier(train_x, train_y)\n",
        "svm.fit()\n",
        "pred_train = svm.predict(train_x)\n",
        "accuracy_train = svm.calculate_accuracy(train_x, train_y, pred_train)\n",
        "pred_valid = svm.predict(valid_x)\n",
        "accuracy_valid = svm.calculate_accuracy(valid_x, valid_y, pred_valid)\n",
        "pred_test = svm.predict(test_x)\n",
        "accuracy_test = svm.calculate_accuracy(test_x, test_y, pred_test)\n",
        "print('Train pred:', pred_train)\n",
        "print('Train acc:', accuracy_train)\n",
        "print('Valid pred:', pred_valid)\n",
        "print('Valid acc:', accuracy_valid)\n",
        "print('Test pred:', pred_test)\n",
        "print('Test acc:', accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYEql8h9nL71",
        "outputId": "c88d5a5c-b46f-42ca-b2ca-966ee6a0651f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pred: [0. 0. 0. ... 0. 0. 0.]\n",
            "Train acc: 0.9288398158878607\n",
            "Valid pred: [0. 0. 0. ... 0. 0. 0.]\n",
            "Valid acc: 0.9298309189418513\n",
            "Test pred: [0. 0. 0. ... 0. 0. 0.]\n",
            "Test acc: 0.9290270233721742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NB\n",
        "class NaiveBayesClassifier():\n",
        "    def __init__(self, train_x, train_y):\n",
        "        \"\"\"\n",
        "        Initialize the NaiveBayesClassifier with training data and model parameters.\n",
        "\n",
        "        Parameters:\n",
        "        - train_x: Features for training.\n",
        "        - train_y: Labels for training.\n",
        "        - priors: Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.\n",
        "        - var_smoothing: Portion of the largest variance of all features that is added to variances for stability.\n",
        "        \"\"\"\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.accuracies = []\n",
        "\n",
        "        self.model = GaussianNB()\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Fit the Naive Bayes model on the training data and track training accuracy.\"\"\"\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "        accuracy = self._calculate_accuracy_internal(self.train_x, self.train_y)\n",
        "        self.accuracies.append(accuracy)\n",
        "\n",
        "    def _calculate_accuracy_internal(self, X, y):\n",
        "        \"\"\"Internal method to predict and calculate accuracy on the training data.\"\"\"\n",
        "        predictions = self.model.predict(X)\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the labels for the input features X.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call `fit` before `predict`.\")\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy of the model on the given data X and true labels y.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Features for prediction.\n",
        "        - y: True labels.\n",
        "        - pred: Precomputed predictions (optional).\n",
        "\n",
        "        Returns:\n",
        "        - accuracy: Accuracy of the predictions.\n",
        "        \"\"\"\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        \"\"\"Plot the training accuracy curve after fitting the model.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o', linestyle='-')\n",
        "        plt.title('Naive Bayes Training Accuracy Curve')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "nb = NaiveBayesClassifier(train_x, train_y)\n",
        "nb.fit()\n",
        "pred_train = nb.predict(train_x)\n",
        "accuracy_train = nb.calculate_accuracy(train_x, train_y, pred_train)\n",
        "pred_valid = nb.predict(valid_x)\n",
        "accuracy_valid = nb.calculate_accuracy(valid_x, valid_y, pred_valid)\n",
        "pred_test = nb.predict(test_x)\n",
        "accuracy_test = nb.calculate_accuracy(test_x, test_y, pred_test)\n",
        "print('Train pred:', pred_train)\n",
        "print('Train acc:', accuracy_train)\n",
        "print('Valid pred:', pred_valid)\n",
        "print('Valid acc:', accuracy_valid)\n",
        "print('Test pred:', pred_test)\n",
        "print('Test acc:', accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSIRxIC7opts",
        "outputId": "a9c67546-ba5b-4259-c9d3-1e3486e1a134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pred: [1. 0. 0. ... 0. 0. 0.]\n",
            "Train acc: 0.9322993272825678\n",
            "Valid pred: [1. 0. 0. ... 0. 0. 0.]\n",
            "Valid acc: 0.9330393049380861\n",
            "Test pred: [0. 0. 1. ... 0. 0. 0.]\n",
            "Test acc: 0.9322800453770275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RF\n",
        "class RandomForestModel():\n",
        "    def __init__(self, train_x, train_y, n_estimators=10, max_depth=None, random_state=42, max_samples=0.05, warm_start=True):\n",
        "        \"\"\"\n",
        "        Initialize the RandomForestModel with training data and hyperparameters.\n",
        "\n",
        "        Parameters:\n",
        "        - train_x: Features for training.\n",
        "        - train_y: Labels for training.\n",
        "        - n_estimators: The number of trees in the forest.\n",
        "        - max_depth: The maximum depth of the tree.\n",
        "        - random_state: Controls the randomness of the estimator.\n",
        "        - max_samples: The number of samples to draw from X to train each base estimator.\n",
        "        - warm_start: When set to True, reuse the solution of the previous call to fit.\n",
        "        \"\"\"\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.accuracies = []\n",
        "\n",
        "        self.model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            random_state=random_state,\n",
        "            max_samples=max_samples,\n",
        "            warm_start=warm_start\n",
        "        )\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Fit the RandomForest model on the training data, tracking accuracy at each step.\"\"\"\n",
        "        for i in range(1, self.model.n_estimators + 1):\n",
        "            self.model.n_estimators = i\n",
        "            self.model.fit(self.train_x, self.train_y)\n",
        "            accuracy = self._calculate_accuracy_internal(self.train_x, self.train_y)\n",
        "            self.accuracies.append(accuracy)\n",
        "\n",
        "    def _calculate_accuracy_internal(self, X, y):\n",
        "        \"\"\"Internal method to predict and calculate accuracy on the training data.\"\"\"\n",
        "        predictions = self.model.predict(X)\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the labels for the input features X.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call `fit` before `predict`.\")\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy of the model on the given data X and true labels y.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Features for prediction.\n",
        "        - y: True labels.\n",
        "        - pred: Precomputed predictions (optional).\n",
        "\n",
        "        Returns:\n",
        "        - accuracy: Accuracy of the predictions.\n",
        "        \"\"\"\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        \"\"\"Plot the training accuracy curve based on the number of trees in the forest.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o', linestyle='-')\n",
        "        plt.title('Random Forest Training Accuracy Curve')\n",
        "        plt.xlabel('Number of Trees')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "rf = RandomForestModel(train_x, train_y)\n",
        "rf.fit()\n",
        "pred_train = rf.predict(train_x)\n",
        "accuracy_train = rf.calculate_accuracy(train_x, train_y, pred_train)\n",
        "pred_valid = rf.predict(valid_x)\n",
        "accuracy_valid = rf.calculate_accuracy(valid_x, valid_y, pred_valid)\n",
        "pred_test = rf.predict(test_x)\n",
        "accuracy_test = rf.calculate_accuracy(test_x, test_y, pred_test)\n",
        "print('Train pred:', pred_train)\n",
        "print('Train acc:', accuracy_train)\n",
        "print('Valid pred:', pred_valid)\n",
        "print('Valid acc:', accuracy_valid)\n",
        "print('Test pred:', pred_test)\n",
        "print('Test acc:', accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gClO2PkCo077",
        "outputId": "56f4d4fd-ce87-43bc-db7a-1ad252b2a132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pred: [1. 0. 0. ... 0. 0. 0.]\n",
            "Train acc: 0.9580304181204777\n",
            "Valid pred: [0. 0. 0. ... 0. 0. 0.]\n",
            "Valid acc: 0.9578137014875244\n",
            "Test pred: [0. 0. 1. ... 0. 0. 0.]\n",
            "Test acc: 0.9568242090949387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#XBG\n",
        "class XGBoostClassifier():\n",
        "    def __init__(self, train_x, train_y, n_estimators=100, learning_rate=0.1, max_depth=6,\n",
        "                 subsample=0.8, colsample_bytree=0.8, gamma=0, reg_alpha=0, reg_lambda=1, eval_metric='error'):\n",
        "        \"\"\"\n",
        "        Initialize the XGBoostClassifier with training data and hyperparameters.\n",
        "\n",
        "        Parameters:\n",
        "        - train_x: Features for training.\n",
        "        - train_y: Labels for training.\n",
        "        - n_estimators: Number of trees in the ensemble.\n",
        "        - learning_rate: Step size shrinkage used to prevent ovexgbitting.\n",
        "        - max_depth: Maximum depth of a tree.\n",
        "        - subsample: Subsample ratio of the training instances.\n",
        "        - colsample_bytree: Subsample ratio of columns when constructing each tree.\n",
        "        - gamma: Minimum loss reduction required to make a further partition.\n",
        "        - reg_alpha: L1 regularization term on weights.\n",
        "        - reg_lambda: L2 regularization term on weights.\n",
        "        - eval_metric: Evaluation metric for cross-validation (default is 'error').\n",
        "        \"\"\"\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "\n",
        "        self.model = XGBClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            max_depth=max_depth,\n",
        "            subsample=subsample,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            gamma=gamma,\n",
        "            reg_alpha=reg_alpha,\n",
        "            reg_lambda=reg_lambda,\n",
        "            eval_metric=eval_metric\n",
        "        )\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Fit the XGBoost model on the training data.\"\"\"\n",
        "        self.model.fit(\n",
        "            self.train_x, self.train_y,\n",
        "            eval_set=[(self.train_x, self.train_y)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the labels for the input features X.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call `fit` before `predict`.\")\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy of the model on the given data X and true labels y.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Features for prediction.\n",
        "        - y: True labels.\n",
        "        - pred: Precomputed predictions (optional).\n",
        "\n",
        "        Returns:\n",
        "        - accuracy: Accuracy of the predictions.\n",
        "        \"\"\"\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        \"\"\"Plot the training accuracy curve based on the model's evaluation results.\"\"\"\n",
        "        evals_result = self.model.evals_result()\n",
        "        error_values = evals_result['validation_0']['error']\n",
        "        accuracy_values = [1 - e for e in error_values]  # Convert error to accuracy\n",
        "        epochs = len(accuracy_values)\n",
        "        x_axis = range(epochs)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(x_axis, accuracy_values, marker='o', linestyle='-')\n",
        "        plt.title('XGBoost Training Accuracy Curve')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "xgb = XGBoostClassifier(train_x, train_y)\n",
        "xgb.fit()\n",
        "pred_train = xgb.predict(train_x)\n",
        "accuracy_train = xgb.calculate_accuracy(train_x, train_y, pred_train)\n",
        "pred_valid = xgb.predict(valid_x)\n",
        "accuracy_valid = xgb.calculate_accuracy(valid_x, valid_y, pred_valid)\n",
        "pred_test = xgb.predict(test_x)\n",
        "accuracy_test = xgb.calculate_accuracy(test_x, test_y, pred_test)\n",
        "print('Train pred:', pred_train)\n",
        "print('Train acc:', accuracy_train)\n",
        "print('Valid pred:', pred_valid)\n",
        "print('Valid acc:', accuracy_valid)\n",
        "print('Test pred:', pred_test)\n",
        "print('Test acc:', accuracy_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nt_B7kNo-td",
        "outputId": "e04a4ca7-41ed-422e-c071-49a2ffd0132d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pred: [1. 0. 0. ... 0. 0. 0.]\n",
            "Train acc: 0.960472976661556\n",
            "Valid pred: [0. 0. 0. ... 0. 0. 0.]\n",
            "Valid acc: 0.9611193113018269\n",
            "Test pred: [0. 0. 1. ... 0. 0. 0.]\n",
            "Test acc: 0.9603627157099176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqCIknAetfip",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cda8634b-5d1c-4288-c137-41c4cd414adc",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training estimator 1/5...\n",
            "We have to download the TabPFN, as there is no checkpoint at  /usr/local/lib/python3.10/dist-packages/tabpfn/models_diff/prior_diff_real_checkpoint_n_0_epoch_100.cpkt\n",
            "It has about 100MB, so this might take a moment.\n",
            "Training estimator 2/5...\n",
            "Training estimator 3/5...\n",
            "Training estimator 4/5...\n",
            "Training estimator 5/5...\n",
            "Bagging model training complete.\n",
            "Predicting with model 1/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicting with model 2/5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=fp16_inference):\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-85a7c50113b8>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mtabpfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabPFNBagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mtabpfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mpred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabpfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0maccuracy_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabpfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mpred_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtabpfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-85a7c50113b8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0my_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_winning_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, return_winning_probability, normalize_with_test)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_winning_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_with_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize_with_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalize_with_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, normalize_with_test, return_logits)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0meval_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         prediction = transformer_predict(self.model[2], X_full, y_full, eval_pos,\n\u001b[0m\u001b[1;32m    274\u001b[0m                                          \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                                          \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py\u001b[0m in \u001b[0;36mtransformer_predict\u001b[0;34m(model, eval_xs, eval_ys, eval_position, device, max_features, style, inference_mode, num_classes, extend_features, normalize_with_test, normalize_to_ranking, softmax_temperature, multiclass_decoder, preprocess_transform, categorical_feats, feature_shift_decoder, N_ensemble_configurations, batch_size_inference, differentiable_hps_as_style, average_logits, fp16_inference, normalize_with_sqrt, seed, no_grad, return_logits, **kwargs)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfp16_inference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                     \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_temperature_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;31m#print('MODEL INFERENCE TIME ('+str(batch_input.device)+' vs '+device+', '+str(fp16_inference)+')', str(time.time()-start))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             )\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/scripts/transformer_prediction_interface.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(eval_xs, eval_ys, used_style, softmax_temperature, return_logits)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0minference_mode_call\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             output = model(\n\u001b[0m\u001b[1;32m    361\u001b[0m                     \u001b[0;34m(\u001b[0m\u001b[0mused_style\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_xs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mused_style\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_ys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                     single_eval_pos=eval_position)[:, :, 0:num_classes]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, single_eval_pos)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msingle_eval_pos\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_src\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_att_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_embeddings\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_att_embeddings\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tabpfn/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0msingle_eval_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0msrc_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0msrc_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msingle_eval_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0msrc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msrc_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_right\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5523\u001b[0m             \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaddbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5525\u001b[0;31m             \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5526\u001b[0m         \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5527\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# @title Testing\n",
        "# class TabPFNBagging():\n",
        "#     def __init__(self, train_x, train_y, n_estimators=10, sample_size=1000, device=device, N_ensemble_configurations=1):\n",
        "#         self.train_x = train_x\n",
        "#         self.train_y = train_y\n",
        "#         self.n_estimators = n_estimators\n",
        "#         self.sample_size = sample_size\n",
        "#         self.device = device\n",
        "#         self.N_ensemble_configurations = N_ensemble_configurations\n",
        "#         self.models = []\n",
        "#         self.X_sample, self.y_sample = None, None\n",
        "\n",
        "#     def fit(self):\n",
        "#         for i in range(self.n_estimators):\n",
        "#             print(f\"Training estimator {i + 1}/{self.n_estimators}...\")\n",
        "#             self.X_sample, self.y_sample = resample(self.train_x, self.train_y, n_samples=self.sample_size)\n",
        "#             model = TabPFNClassifier(device=self.device, N_ensemble_configurations=self.N_ensemble_configurations)\n",
        "#             model.fit(self.X_sample, self.y_sample, overwrite_warning=True)\n",
        "#             self.models.append(model)\n",
        "#         print(\"Bagging model training complete.\")\n",
        "\n",
        "#     def predict(self, X, batch_size=9000):\n",
        "#         n_samples = X.shape[0]\n",
        "#         n_batches = int(np.ceil(n_samples / batch_size))\n",
        "\n",
        "#         # Initialize predictions array\n",
        "#         predictions = np.zeros((n_samples, len(self.models)))\n",
        "\n",
        "#         for i, model in enumerate(self.models):\n",
        "#             print(f\"Predicting with model {i+1}/{self.n_estimators}...\")\n",
        "#             for batch_idx in range(n_batches):\n",
        "#                 start_idx = batch_idx * batch_size\n",
        "#                 end_idx = min(start_idx + batch_size, n_samples)\n",
        "#                 X_batch = X[start_idx:end_idx]\n",
        "\n",
        "#                 y_eval, prob = model.predict(X_batch, return_winning_probability=True)\n",
        "#                 predictions[start_idx:end_idx, i] = y_eval\n",
        "\n",
        "#         # Majority voting\n",
        "#         y_final = np.round(np.mean(predictions, axis=1)).astype(int)\n",
        "#         return y_final\n",
        "\n",
        "#     def calculate_accuracy(self, X, y, pred=None):\n",
        "#         y_pred = self.predict(X) if pred is None else pred\n",
        "#         accuracy = accuracy_score(y, y_pred)\n",
        "#         print(f\"Bagging model accuracy: {accuracy:.4f}\")\n",
        "#         return accuracy\n",
        "\n",
        "# tabpfn = TabPFNBagging(train_x, train_y)\n",
        "# tabpfn.fit()\n",
        "# pred_train = tabpfn.predict(train_x)\n",
        "# accuracy_train = tabpfn.calculate_accuracy(train_x, train_y, pred_train)\n",
        "# pred_valid = tabpfn.predict(valid_x)\n",
        "# accuracy_valid = tabpfn.calculate_accuracy(valid_x, valid_y, pred_valid)\n",
        "# pred_test = tabpfn.predict(test_x)\n",
        "# accuracy_test = tabpfn.calculate_accuracy(test_x, test_y, pred_test)\n",
        "# print('Train pred:', pred_train)\n",
        "# print('Train acc:', accuracy_train)\n",
        "# print('Valid pred:', pred_valid)\n",
        "# print('Valid acc:', accuracy_valid)\n",
        "# print('Test pred:', pred_test)\n",
        "# print('Test acc:', accuracy_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fkw5jbXSIpfK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Testing 2\n",
        "class Adaboost():\n",
        "    def __init__(self, classes_dict, train_x, train_y):\n",
        "        self.classes_dict = classes_dict\n",
        "        self.model_order = list(classes_dict.keys())\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.trained_model = {}\n",
        "        self.training_data_history = {'base': {'X': self.train_x, 'y': self.train_y}}\n",
        "        self.current_weight = None\n",
        "        self.weight_history = {}\n",
        "        self.restart = False\n",
        "\n",
        "    def weight_init(self): self.current_weight = pd.Series(np.ones(len(self.train_y)) / len(self.train_y))\n",
        "\n",
        "    def weight_calculate(self, predictions, labels):\n",
        "        incorrect = predictions != labels.to_numpy()\n",
        "        error_rate = self.current_weight[incorrect].sum()\n",
        "\n",
        "        print('Error rate is:', error_rate)\n",
        "\n",
        "        if error_rate > 0.5:\n",
        "            self.weight_init()\n",
        "            self.restart = True\n",
        "            return\n",
        "\n",
        "        alpha = 0.5 * np.log((1 - error_rate) / error_rate)\n",
        "\n",
        "        # Update weights\n",
        "        self.current_weight[incorrect] *= np.exp(alpha)\n",
        "        self.current_weight[~incorrect] *= np.exp(-alpha)\n",
        "\n",
        "        # Normalize weights\n",
        "        self.current_weight /= self.current_weight.sum()\n",
        "\n",
        "    def training(self):\n",
        "        self.weight_init()\n",
        "        for model in self.model_order:\n",
        "            while True:\n",
        "                self.restart = False\n",
        "                self.train_x['weight'] = self.current_weight\n",
        "                self.train_y = self.train_y.to_frame()\n",
        "                self.train_y['weight'] = self.current_weight\n",
        "                sampled_train_x = self.train_x.sample(n=len(self.train_x), replace=True, weights='weight', random_state=42)\n",
        "                del self.train_x['weight']\n",
        "                del sampled_train_x['weight']\n",
        "                sampled_train_x.sort_index(inplace=True)\n",
        "                sampled_train_x.reset_index(drop=True, inplace=True)\n",
        "                sampled_train_y = self.train_y.sample(n=len(self.train_y), replace=True, weights='weight', random_state=42)\n",
        "                del self.train_y['weight']\n",
        "                self.train_y = self.train_y.iloc[:, 0]\n",
        "                del sampled_train_y['weight']\n",
        "                sampled_train_y.sort_index(inplace=True)\n",
        "                sampled_train_y.reset_index(drop=True, inplace=True)\n",
        "                sampled_train_y = sampled_train_y.iloc[:, 0]\n",
        "\n",
        "                print(\"*\" * 37)\n",
        "                print(f'Training --------------------- {model}')\n",
        "                current_model = self.classes_dict[model](sampled_train_x, sampled_train_y)\n",
        "                methods = inspect.getmembers(current_model, predicate=inspect.ismethod)\n",
        "\n",
        "                if 'fit' in [z for z, _ in methods]:\n",
        "                    current_model.fit()\n",
        "                    print('Finish training.\\nStart predicting.')\n",
        "                    current_prediction = current_model.predict(current_model.train_x)\n",
        "                    train_accuracy = current_model.calculate_accuracy(current_model.train_x , current_model.train_y, current_prediction)\n",
        "                else:\n",
        "                    current_model.to(device)\n",
        "                    train_model_pt(current_model)\n",
        "                    print('Finish training.\\nStart predicting.')\n",
        "                    current_prediction = predict_model_pt(current_model, current_model.train_x)\n",
        "                    train_accuracy = calculate_accuracy_pt(current_model, current_model.train_x, current_model.train_y, current_prediction)\n",
        "\n",
        "                print(f'{model} training accuracy:', train_accuracy)\n",
        "                self.weight_calculate(current_prediction, sampled_train_y)\n",
        "\n",
        "                if not self.restart:\n",
        "                    self.train_x, self.train_y = sampled_train_x, sampled_train_y\n",
        "                    self.training_data_history[model] = {'X': sampled_train_x, 'y': sampled_train_y}\n",
        "                    self.trained_model[model] = current_model\n",
        "                    break\n",
        "    def reorder(self, order_list): self.model_order = order_list\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Ensure X is a pandas DataFrame\n",
        "        assert isinstance(X, pd.DataFrame), \"Input X should be a pandas DataFrame\"\n",
        "        # Collect predictions from each trained model\n",
        "        model_predictions = {}\n",
        "        for model_name, model in self.trained_model.items():\n",
        "            methods = inspect.getmembers(model, predicate=inspect.ismethod)\n",
        "            if 'predict' in [z for z, _ in methods]: preds = model.predict(X)\n",
        "            else: preds = predict_model_pt(model, torch.tensor(X.to_numpy(), dtype=torch.float32).to(device))\n",
        "            assert isinstance(preds, np.ndarray), f\"Predictions from {model_name} should be a numpy array\"\n",
        "            if not np.array_equal(np.unique(preds), [0, 1]):\n",
        "                print('Alert --- ', np.unique(preds))\n",
        "                preds = np.where(preds > 0.5, 1, 0)\n",
        "            model_predictions[model_name] = preds\n",
        "        # Voting mechanism\n",
        "        predictions = np.zeros(len(X))\n",
        "        for i in range(len(X)):\n",
        "            votes = {}\n",
        "            for model_name, preds in model_predictions.items():\n",
        "                pred = preds[i]\n",
        "                if pred in votes: votes[pred] += 1\n",
        "                else: votes[pred] = 1\n",
        "            predictions[i] = max(votes, key=votes.get)\n",
        "        assert isinstance(predictions, np.ndarray), \"Final predictions should be a numpy array\"\n",
        "        return predictions\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        preds = pred if pred is not None else self.predict(X)\n",
        "        assert isinstance(preds, np.ndarray), \"Predictions should be a numpy array\"\n",
        "        assert isinstance(y, pd.Series), \"y should be a pandas Series\"\n",
        "        assert len(pred) == len(y), \"Predictions and y should have the same length\"\n",
        "        accuracy = np.mean(preds == y.to_numpy())\n",
        "        return accuracy\n",
        "adModel = Adaboost(classes_dict={\"tabpfn\":TabPFNBagging , \"lr\": LogisticRegressionModel, \"svm\": SVMClassifier, \"ann\": ANN, \"nb\": NaiveBayesClassifier, \"rf\": RandomForestModel, \"xgb\": XGBoostClassifier}, train_x=train_x, train_y=train_y)\n",
        "adModel.reorder(['svm', 'xgb', 'ann', 'rf', 'tabpfn'])\n",
        "adModel.training()\n",
        "print(\"*\" * 37)\n",
        "train_predict = adModel.predict(train_x)\n",
        "train_acc = adModel.calculate_accuracy(train_x, train_y, train_predict)\n",
        "test_predict = adModel.predict(test_x)\n",
        "test_acc = adModel.calculate_accuracy(test_x, test_y, test_predict)\n",
        "print(\"Adaboost model prediction: \", train_predict)\n",
        "print(\"Adaboost model training accuracy: \", train_acc)\n",
        "print(\"Adaboost model prediction: \", test_predict)\n",
        "print(\"Adaboost model training accuracy: \", test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ2vWXWUydAs",
        "collapsed": true
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9wYZWNvWvKig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDj7M0P0A_HT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}