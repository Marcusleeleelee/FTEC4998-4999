{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcusleeleelee/FTEC4998-4999/blob/main/FTEC4998_4999.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Irism69Bo6L",
        "outputId": "037adcae-3e35-4797-ab78-3ccef96b1392"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Basic libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn models and utilities\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Scikit-learn utilities\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# PyTorch for neural network models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Progress bar for loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Plotting accuracy curves and other metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Others\n",
        "import inspect\n",
        "# Google Colab specific (if needed)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ndu77M8kBr-h"
      },
      "outputs": [],
      "source": [
        "# Step 1: Utils - ok\n",
        "def uni_list(input): return list(set(input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "lmLx5bbgBtcq"
      },
      "outputs": [],
      "source": [
        "class Dataset():\n",
        "    def __init__(self, file_path):\n",
        "        self.dataset = pd.read_feather(file_path)\n",
        "        self.original = self.dataset.copy()\n",
        "        self.X_train, self.y_train = None, None\n",
        "        self.X_test, self.y_test = None, None\n",
        "        self.label = 'loan_condition_cat'\n",
        "        self.min_max_columns = ['annual_inc', 'year']\n",
        "        self.means = {}\n",
        "        self.stds = {}\n",
        "        self.mins = {}\n",
        "        self.maxs = {}\n",
        "\n",
        "    def show(self, rows=10):\n",
        "        return self.dataset.head(rows)\n",
        "\n",
        "    def basic_processing(self):\n",
        "        temp_func_2 = lambda x: {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G': 7}[str(x)]\n",
        "        columns_to_delete = [\n",
        "            'id', 'issue_d', 'home_ownership_cat', 'income_category', 'income_cat', 'term_cat', 'application_type_cat',\n",
        "            'purpose_cat', 'interest_payment_cat', 'loan_condition'\n",
        "        ]\n",
        "        self.dataset.drop(columns=columns_to_delete, inplace=True)\n",
        "        self.dataset['grade'] = self.dataset['grade'].apply(temp_func_2)\n",
        "        self.dataset['final_d'] = self.dataset['final_d'].apply(lambda x: str(x)[-4:]).apply(int)\n",
        "        self.dataset['year'] = self.dataset['year'].apply(lambda x: str(x)[-4:]).apply(int)\n",
        "        self.dataset = pd.get_dummies(self.dataset, columns=['home_ownership', 'term', 'application_type',\n",
        "                                                             'purpose', 'interest_payments', 'region'], dtype=int)\n",
        "\n",
        "\n",
        "    def train_test_split(self, test_size=0.2, random_state=42):\n",
        "        X = self.dataset.drop(columns=[self.label])\n",
        "        y = self.dataset[self.label]\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "        self.original_columns = X.columns\n",
        "\n",
        "        # Sort by index\n",
        "        self.X_train.sort_index(inplace=True)\n",
        "        self.X_train.reset_index(drop=True, inplace=True)\n",
        "        self.X_test.sort_index(inplace=True)\n",
        "        self.X_test.reset_index(drop=True, inplace=True)\n",
        "        self.y_train.sort_index(inplace=True)\n",
        "        self.y_train.reset_index(drop=True, inplace=True)\n",
        "        self.y_test.sort_index(inplace=True)\n",
        "        self.y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def preprocessing_train(self):\n",
        "        # Separate columns for Min-Max and Z-score normalization\n",
        "        columns_to_normalize = self.min_max_columns\n",
        "        columns_to_scale = [col for col in self.X_train.columns if col not in columns_to_normalize]\n",
        "\n",
        "        # Z-score normalization\n",
        "        for col in columns_to_scale:\n",
        "            mean = np.mean(self.X_train[col])\n",
        "            std = np.std(self.X_train[col])\n",
        "            self.means[col] = mean\n",
        "            self.stds[col] = std\n",
        "            self.X_train[col] = (self.X_train[col] - mean) / std\n",
        "\n",
        "        # Min-Max normalization\n",
        "        for col in columns_to_normalize:\n",
        "            min_val = np.min(self.X_train[col])\n",
        "            max_val = np.max(self.X_train[col])\n",
        "            self.mins[col] = min_val\n",
        "            self.maxs[col] = max_val\n",
        "            self.X_train[col] = (self.X_train[col] - min_val) / (max_val - min_val)\n",
        "\n",
        "        # Perform PCA\n",
        "        selected_columns = self.perform_pca(self.X_train, n_components=35)\n",
        "        self.X_train = self.X_train[selected_columns]\n",
        "\n",
        "\n",
        "    def perform_pca(self, data, n_components):\n",
        "        # Center the data\n",
        "        data_mean = np.mean(data, axis=0)\n",
        "        centered_data = data - data_mean\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov_matrix = np.cov(centered_data, rowvar=False)\n",
        "\n",
        "        # Eigen decomposition\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "        # Sort eigenvectors by eigenvalues in descending order\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "        # Select the top n_components\n",
        "        selected_eigenvectors = sorted_eigenvectors[:, :n_components]\n",
        "\n",
        "        # Identify important features\n",
        "        feature_importance = np.abs(selected_eigenvectors).sum(axis=1)\n",
        "        important_indices = np.argsort(feature_importance)[::-1][:n_components]\n",
        "\n",
        "        # Return the original column names of these features\n",
        "        important_features = [self.original_columns[i] for i in important_indices]\n",
        "\n",
        "        return important_features\n",
        "\n",
        "    def preprocessing_test(self):\n",
        "        # Separate columns for Min-Max and Z-score normalization\n",
        "        columns_to_normalize = self.min_max_columns\n",
        "        columns_to_scale = [col for col in self.X_test.columns if col not in columns_to_normalize]\n",
        "\n",
        "        # Apply Z-score normalization using training statistics\n",
        "        for col in columns_to_scale:\n",
        "            self.X_test[col] = (self.X_test[col] - self.means[col]) / self.stds[col]\n",
        "\n",
        "        # Apply Min-Max normalization using training statistics\n",
        "        for col in columns_to_normalize:\n",
        "            self.X_test[col] = (self.X_test[col] - self.mins[col]) / (self.maxs[col] - self.mins[col])\n",
        "\n",
        "        # Apply PCA using training components\n",
        "        self.X_test = self.X_test[[i for i in self.X_train.columns.to_list()]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "13kLSUwPBvO9"
      },
      "outputs": [],
      "source": [
        "# Calculating # ok\n",
        "data = Dataset('/content/drive/My Drive/Colab Notebooks/FTEC4998_9/loan_final313_processed.feather')\n",
        "data.basic_processing()\n",
        "data.train_test_split()\n",
        "data.preprocessing_train()\n",
        "data.preprocessing_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMPCAhUjB4tJ",
        "outputId": "d13a955d-3c6e-405b-c81e-c24e9ac6b94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.5910370853482805\n",
            "(709903, 35) (709903,)\n",
            "(177476, 35) (177476,)\n"
          ]
        }
      ],
      "source": [
        "# Data conversion # ok\n",
        "train_x, train_y = data.X_train, data.y_train\n",
        "test_x, test_y = data.X_test, data.y_test\n",
        "counts = np.mean(train_y == 1) * 100\n",
        "print(counts)\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(test_x.shape, test_y.shape)\n",
        "# Ensure y_train is binary\n",
        "assert set(train_y).issubset({0, 1}), \"Target values must be 0 or 1 for binary classification.\"\n",
        "# Move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x.shape); print(train_y.shape)\n",
        "print(test_x.shape); print(test_y.shape)"
      ],
      "metadata": {
        "id": "6sYQkDstjfk4",
        "outputId": "da0ef148-954c-408d-cd91-92c4a9a8ce30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(709903, 35)\n",
            "(709903,)\n",
            "(177476, 35)\n",
            "(177476,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "5UL36a0-Tq97"
      },
      "outputs": [],
      "source": [
        "# Train, predict, and accuracy functions\n",
        "def train_model_pt(model): # ok\n",
        "    model.train()\n",
        "    for epoch in range(model.epochs):\n",
        "        model.optimizer.zero_grad()\n",
        "        outputs = model(model.train_x)\n",
        "        loss = model.criterion(outputs, model.train_y)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "        if epoch % 5 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "def predict_model_pt(model, X):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X = X.to(next(model.parameters()).device)\n",
        "        outputs = model(X).squeeze()\n",
        "        return (outputs > 0.5).float().cpu().numpy()  # Convert to numpy array\n",
        "def calculate_accuracy_pt(model, X, y, pred=None):\n",
        "    # Ensure X and y are on the correct device\n",
        "    X = X.to(next(model.parameters()).device)\n",
        "    y = y.to(next(model.parameters()).device)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = predict_model_pt(model, X) if pred is None else pred\n",
        "\n",
        "    # Ensure predictions and labels are tensors and the same shape\n",
        "    predictions = predictions.squeeze()\n",
        "    y = y.squeeze()\n",
        "\n",
        "    # Convert to tensors if necessary\n",
        "    if not isinstance(predictions, torch.Tensor): predictions = torch.tensor(predictions)\n",
        "    if not isinstance(y, torch.Tensor): y = torch.tensor(y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct = (predictions == y).sum().item()\n",
        "    accuracy = correct / len(y)\n",
        "    return accuracy\n",
        "\n",
        "def df_to_tensor(x, y):\n",
        "    assert isinstance(x, pd.DataFrame) and isinstance(y, pd.Series)\n",
        "    return torch.tensor(x.to_numpy(), dtype=torch.float32).to(device), torch.tensor(y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "test_x, test_y = data.X_test, data.y_test\n",
        "test_x_tensor, test_y_tensor = df_to_tensor(data.X_test, data.y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "bqCIknAetfip"
      },
      "outputs": [],
      "source": [
        "# MLP model\n",
        "class ANN(nn.Module): # Not yet finish the plot function\n",
        "    def __init__(self, train_x, train_y, lr=0.01):\n",
        "        super(ANN, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 50\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "# Logistic Regression as a neural network\n",
        "class LogisticRegressionModel(nn.Module): # Not yet finish the plot function\n",
        "    def __init__(self, train_x, train_y, lr=0.001):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 1200\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# SVM\n",
        "class SVMClassifier():\n",
        "    def __init__(self, train_x, train_y, fraction=0.1, C=0.1, kernel='poly', degree=3, gamma='scale', n_estimators=6, max_samples=0.001, random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the SVMClassifier with training data and optional parameters.\n",
        "\n",
        "        Parameters:\n",
        "        - train_x: Features for training.\n",
        "        - train_y: Labels for training.\n",
        "        - fraction: Fraction of data to sample (not used in this implementation).\n",
        "        - C: Penalty parameter C of the error term for SVM.\n",
        "        - kernel: Kernel type to be used in the SVM algorithm.\n",
        "        - degree: Degree of the polynomial kernel function (if kernel='poly').\n",
        "        - gamma: Kernel coefficient for 'rbf', 'poly', and 'sigmoid'.\n",
        "        - n_estimators: The number of base estimators in the Bagging ensemble.\n",
        "        - max_samples: The number of samples to draw from X to train each base estimator.\n",
        "        - random_state: Controls the randomness of the bootstrapping of the samples.\n",
        "        \"\"\"\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.C = C\n",
        "        self.kernel = kernel\n",
        "        self.degree = degree\n",
        "        self.gamma = gamma\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_samples = max_samples\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.model = None\n",
        "        self.accuracies = []\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Fits the SVM model using Bagging with the specified parameters.\"\"\"\n",
        "        self.model = BaggingClassifier(\n",
        "            estimator=SVC(C=self.C, kernel=self.kernel, degree=self.degree, gamma=self.gamma),\n",
        "            n_estimators=self.n_estimators,\n",
        "            max_samples=self.max_samples,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "        self._update_accuracy()\n",
        "\n",
        "    def _update_accuracy(self):\n",
        "        \"\"\"Internal method to update and store the model's accuracy on the training set.\"\"\"\n",
        "        predictions = self.model.predict(self.train_x)\n",
        "        accuracy = accuracy_score(self.train_y, predictions)\n",
        "        self.accuracies.append(accuracy)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predicts the labels for the given input data X.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call `fit` before `predict`.\")\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        \"\"\"Calculates the accuracy of the model on the given data X and true labels y.\"\"\"\n",
        "        if pred is None:\n",
        "            pred = self.predict(X)\n",
        "        return accuracy_score(y, pred)\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        \"\"\"Plots the training accuracy curve after fitting the model.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o', linestyle='-')\n",
        "        plt.title('SVM Training Accuracy Curve')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# NB\n",
        "class NaiveBayesClassifier():\n",
        "    def __init__(self, train_x, train_y, priors=None, var_smoothing=1e-9):\n",
        "        \"\"\"\n",
        "        Initialize the NaiveBayesClassifier with training data and model parameters.\n",
        "\n",
        "        Parameters:\n",
        "        - train_x: Features for training.\n",
        "        - train_y: Labels for training.\n",
        "        - priors: Prior probabilities of the classes. If specified, the priors are not adjusted according to the data.\n",
        "        - var_smoothing: Portion of the largest variance of all features that is added to variances for stability.\n",
        "        \"\"\"\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.accuracies = []\n",
        "\n",
        "        self.model = GaussianNB(\n",
        "            priors=priors,\n",
        "            var_smoothing=var_smoothing\n",
        "        )\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Fit the Naive Bayes model on the training data and track training accuracy.\"\"\"\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "        accuracy = self._calculate_accuracy_internal(self.train_x, self.train_y)\n",
        "        self.accuracies.append(accuracy)\n",
        "\n",
        "    def _calculate_accuracy_internal(self, X, y):\n",
        "        \"\"\"Internal method to predict and calculate accuracy on the training data.\"\"\"\n",
        "        predictions = self.model.predict(X)\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the labels for the input features X.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call `fit` before `predict`.\")\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy of the model on the given data X and true labels y.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Features for prediction.\n",
        "        - y: True labels.\n",
        "        - pred: Precomputed predictions (optional).\n",
        "\n",
        "        Returns:\n",
        "        - accuracy: Accuracy of the predictions.\n",
        "        \"\"\"\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        \"\"\"Plot the training accuracy curve after fitting the model.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o', linestyle='-')\n",
        "        plt.title('Naive Bayes Training Accuracy Curve')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# RF\n",
        "class RandomForestModel():\n",
        "    def __init__(self, train_x, train_y, n_estimators=10, max_depth=None, random_state=42, max_samples=0.05, warm_start=True):\n",
        "        \"\"\"\n",
        "        Initialize the RandomForestModel with training data and hyperparameters.\n",
        "\n",
        "        Parameters:\n",
        "        - train_x: Features for training.\n",
        "        - train_y: Labels for training.\n",
        "        - n_estimators: The number of trees in the forest.\n",
        "        - max_depth: The maximum depth of the tree.\n",
        "        - random_state: Controls the randomness of the estimator.\n",
        "        - max_samples: The number of samples to draw from X to train each base estimator.\n",
        "        - warm_start: When set to True, reuse the solution of the previous call to fit.\n",
        "        \"\"\"\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.accuracies = []\n",
        "\n",
        "        self.model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            random_state=random_state,\n",
        "            max_samples=max_samples,\n",
        "            warm_start=warm_start\n",
        "        )\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Fit the RandomForest model on the training data, tracking accuracy at each step.\"\"\"\n",
        "        for i in range(1, self.model.n_estimators + 1):\n",
        "            self.model.n_estimators = i\n",
        "            self.model.fit(self.train_x, self.train_y)\n",
        "            accuracy = self._calculate_accuracy_internal(self.train_x, self.train_y)\n",
        "            self.accuracies.append(accuracy)\n",
        "\n",
        "    def _calculate_accuracy_internal(self, X, y):\n",
        "        \"\"\"Internal method to predict and calculate accuracy on the training data.\"\"\"\n",
        "        predictions = self.model.predict(X)\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the labels for the input features X.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call `fit` before `predict`.\")\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy of the model on the given data X and true labels y.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Features for prediction.\n",
        "        - y: True labels.\n",
        "        - pred: Precomputed predictions (optional).\n",
        "\n",
        "        Returns:\n",
        "        - accuracy: Accuracy of the predictions.\n",
        "        \"\"\"\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        \"\"\"Plot the training accuracy curve based on the number of trees in the forest.\"\"\"\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o', linestyle='-')\n",
        "        plt.title('Random Forest Training Accuracy Curve')\n",
        "        plt.xlabel('Number of Trees')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "#XBG\n",
        "class XGBoostClassifier():\n",
        "    def __init__(self, train_x, train_y, n_estimators=100, learning_rate=0.1, max_depth=6,\n",
        "                 subsample=0.8, colsample_bytree=0.8, gamma=0, reg_alpha=0, reg_lambda=1, eval_metric='error'):\n",
        "        \"\"\"\n",
        "        Initialize the XGBoostClassifier with training data and hyperparameters.\n",
        "\n",
        "        Parameters:\n",
        "        - train_x: Features for training.\n",
        "        - train_y: Labels for training.\n",
        "        - n_estimators: Number of trees in the ensemble.\n",
        "        - learning_rate: Step size shrinkage used to prevent overfitting.\n",
        "        - max_depth: Maximum depth of a tree.\n",
        "        - subsample: Subsample ratio of the training instances.\n",
        "        - colsample_bytree: Subsample ratio of columns when constructing each tree.\n",
        "        - gamma: Minimum loss reduction required to make a further partition.\n",
        "        - reg_alpha: L1 regularization term on weights.\n",
        "        - reg_lambda: L2 regularization term on weights.\n",
        "        - eval_metric: Evaluation metric for cross-validation (default is 'error').\n",
        "        \"\"\"\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "\n",
        "        self.model = XGBClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            max_depth=max_depth,\n",
        "            subsample=subsample,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            gamma=gamma,\n",
        "            reg_alpha=reg_alpha,\n",
        "            reg_lambda=reg_lambda,\n",
        "            eval_metric=eval_metric\n",
        "        )\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"Fit the XGBoost model on the training data.\"\"\"\n",
        "        self.model.fit(\n",
        "            self.train_x, self.train_y,\n",
        "            eval_set=[(self.train_x, self.train_y)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict the labels for the input features X.\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model has not been trained yet. Call `fit` before `predict`.\")\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        \"\"\"\n",
        "        Calculate the accuracy of the model on the given data X and true labels y.\n",
        "\n",
        "        Parameters:\n",
        "        - X: Features for prediction.\n",
        "        - y: True labels.\n",
        "        - pred: Precomputed predictions (optional).\n",
        "\n",
        "        Returns:\n",
        "        - accuracy: Accuracy of the predictions.\n",
        "        \"\"\"\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        return accuracy_score(y, predictions)\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        \"\"\"Plot the training accuracy curve based on the model's evaluation results.\"\"\"\n",
        "        evals_result = self.model.evals_result()\n",
        "        error_values = evals_result['validation_0']['error']\n",
        "        accuracy_values = [1 - e for e in error_values]  # Convert error to accuracy\n",
        "        epochs = len(accuracy_values)\n",
        "        x_axis = range(epochs)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(x_axis, accuracy_values, marker='o', linestyle='-')\n",
        "        plt.title('XGBoost Training Accuracy Curve')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "collapsed": true,
        "id": "fkw5jbXSIpfK"
      },
      "outputs": [],
      "source": [
        "class Adaboost():\n",
        "    def __init__(self, classes_dict, train_x, train_y):\n",
        "        self.classes_dict = classes_dict\n",
        "        self.model_order = list(classes_dict.keys())\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.trained_model = {}\n",
        "        self.training_data_history = {'base': {'X': self.train_x, 'y': self.train_y}}\n",
        "        self.current_weight = None\n",
        "        self.weight_history = {}\n",
        "        self.restart = False\n",
        "        self.predictions = None\n",
        "\n",
        "    def weight_init(self): self.current_weight = pd.Series(np.ones(len(self.train_y)) / len(self.train_y))\n",
        "\n",
        "    def weight_calculate(self, predictions, labels):\n",
        "        incorrect = predictions != labels.to_numpy()\n",
        "        error_rate = self.current_weight[incorrect].sum()\n",
        "\n",
        "        print('Error rate is:', error_rate)\n",
        "\n",
        "        if error_rate > 0.5:\n",
        "            self.weight_init()\n",
        "            self.restart = True\n",
        "            return\n",
        "\n",
        "        alpha = 0.5 * np.log((1 - error_rate) / error_rate)\n",
        "\n",
        "        # Update weights\n",
        "        self.current_weight[incorrect] *= np.exp(alpha)\n",
        "        self.current_weight[~incorrect] *= np.exp(-alpha)\n",
        "\n",
        "        # Normalize weights\n",
        "        self.current_weight /= self.current_weight.sum()\n",
        "\n",
        "    def training(self):\n",
        "        self.weight_init()\n",
        "        for model in self.model_order:\n",
        "            while True:\n",
        "                self.restart = False\n",
        "                self.train_x['weight'] = self.current_weight\n",
        "                self.train_y = self.train_y.to_frame()\n",
        "                self.train_y['weight'] = self.current_weight\n",
        "                sampled_train_x = self.train_x.sample(n=len(self.train_x), replace=True, weights='weight', random_state=42)\n",
        "                del self.train_x['weight']\n",
        "                del sampled_train_x['weight']\n",
        "                sampled_train_x.sort_index(inplace=True)\n",
        "                sampled_train_x.reset_index(drop=True, inplace=True)\n",
        "                sampled_train_y = self.train_y.sample(n=len(self.train_y), replace=True, weights='weight', random_state=42)\n",
        "                del self.train_y['weight']\n",
        "                self.train_y = self.train_y.iloc[:, 0]\n",
        "                del sampled_train_y['weight']\n",
        "                sampled_train_y.sort_index(inplace=True)\n",
        "                sampled_train_y.reset_index(drop=True, inplace=True)\n",
        "                sampled_train_y = sampled_train_y.iloc[:, 0]\n",
        "\n",
        "                print(\"*\" * 37)\n",
        "                print(f'Training --------------------- {model}')\n",
        "                current_model = self.classes_dict[model](sampled_train_x, sampled_train_y)\n",
        "                methods = inspect.getmembers(current_model, predicate=inspect.ismethod)\n",
        "\n",
        "                if 'fit' in [z for z, _ in methods]:\n",
        "                    current_model.fit()\n",
        "                    print('Finish training.\\nStart predicting.')\n",
        "                    current_prediction = current_model.predict(current_model.train_x)\n",
        "                    train_accuracy = current_model.calculate_accuracy(current_model.train_x , current_model.train_y, current_prediction)\n",
        "                else:\n",
        "                    current_model.to(device)\n",
        "                    train_model_pt(current_model)\n",
        "                    print('Finish training.\\nStart predicting.')\n",
        "                    current_prediction = predict_model_pt(current_model, current_model.train_x)\n",
        "                    train_accuracy = calculate_accuracy_pt(current_model, current_model.train_x, current_model.train_y, current_prediction)\n",
        "\n",
        "                print(f'{model} training accuracy:', train_accuracy)\n",
        "                self.weight_calculate(current_prediction, sampled_train_y)\n",
        "\n",
        "                if not self.restart:\n",
        "                    self.train_x, self.train_y = sampled_train_x, sampled_train_y\n",
        "                    self.training_data_history[model] = {'X': sampled_train_x, 'y': sampled_train_y}\n",
        "                    self.trained_model[model] = current_model\n",
        "                    break\n",
        "    def reorder(self, order_list): self.model_order = order_list\n",
        "    def predict(self, X):\n",
        "        # Ensure X is a pandas DataFrame\n",
        "        assert isinstance(X, pd.DataFrame), \"Input X should be a pandas DataFrame\"\n",
        "        # Collect predictions from each trained model\n",
        "        model_predictions = {}\n",
        "        for model_name, model in self.trained_model.items():\n",
        "            methods = inspect.getmembers(model, predicate=inspect.ismethod)\n",
        "            if 'predict' in [z for z, _ in methods]: preds = model.predict(X)\n",
        "            else: preds = predict_model_pt(model, torch.tensor(X.to_numpy(), dtype=torch.float32).to(device))\n",
        "            assert isinstance(preds, np.ndarray), f\"Predictions from {model_name} should be a numpy array\"\n",
        "            if not np.array_equal(np.unique(preds), [0, 1]):\n",
        "                print('Alert --- ', np.unique(preds))\n",
        "                preds = np.where(preds > 0.5, 1, 0)\n",
        "            model_predictions[model_name] = preds\n",
        "        # Voting mechanism\n",
        "        predictions = np.zeros(len(X))\n",
        "        for i in range(len(X)):\n",
        "            votes = {}\n",
        "            for model_name, preds in model_predictions.items():\n",
        "                pred = preds[i]\n",
        "                if pred in votes: votes[pred] += 1\n",
        "                else: votes[pred] = 1\n",
        "            predictions[i] = max(votes, key=votes.get)\n",
        "        assert isinstance(predictions, np.ndarray), \"Final predictions should be a numpy array\"\n",
        "\n",
        "        self.predictions = predictions\n",
        "        return predictions\n",
        "    def calculate_accuracy(self, y):\n",
        "        assert isinstance(self.predictions, np.ndarray), \"Predictions should be a numpy array\"\n",
        "        assert isinstance(y, pd.Series), \"y should be a pandas Series\"\n",
        "        assert len(self.predictions) == len(y), \"Predictions and y should have the same length\"\n",
        "        accuracy = np.mean(self.predictions == y.to_numpy())\n",
        "        return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ2vWXWUydAs",
        "outputId": "65423c99-8c66-4e7d-8a9c-493f0a10982d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************************************\n",
            "Training --------------------- SVM\n"
          ]
        }
      ],
      "source": [
        "adModel = Adaboost(classes_dict={\"LR\": LogisticRegressionModel, \"SVM\": SVMClassifier, \"ANN\": ANN, \"NB\": NaiveBayesClassifier, \"RF\": RandomForestModel, \"XGB\": XGBoostClassifier}, train_x=train_x, train_y=train_y)\n",
        "adModel.reorder(['SVM', 'XGB', 'ANN', 'RF'])\n",
        "adModel.training()\n",
        "print(\"*\" * 37)\n",
        "print(\"Adaboost model prediction: \", adModel.predict(train_x))\n",
        "print(\"Adaboost model training accuracy: \", adModel.calculate_accuracy(train_y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9wYZWNvWvKig"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}