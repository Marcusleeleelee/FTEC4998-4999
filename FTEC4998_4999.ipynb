{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIUOeksZkF6fFMAgK2gZao",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcusleeleelee/FTEC4998-4999/blob/main/FTEC4998_4999.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import inspect\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9Irism69Bo6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b299941a-c0c0-4898-ef64-3834b5344642"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Utils - ok\n",
        "def uni_list(input): return list(set(input))"
      ],
      "metadata": {
        "id": "ndu77M8kBr-h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "    def __init__(self, file_path):\n",
        "        self.dataset = pd.read_feather(file_path)\n",
        "        self.X_train, self.y_train = None, None\n",
        "        self.X_test, self.y_test = None, None\n",
        "        self.scalers = None\n",
        "        self.pca = None\n",
        "        self.label = 'loan_condition_cat'\n",
        "        self.original_columns = None\n",
        "\n",
        "    def show(self, rows=10):\n",
        "        return self.dataset.head(rows)\n",
        "\n",
        "    def basic_processing(self):\n",
        "        temp_func_1 = lambda x: '<=2009' if str(x) in ['2007', '2008', '2009'] else (\"[2010, 2012]\" if str(x) in ['2010', '2011', '2012'] else '>=2013')\n",
        "        columns_to_delete = [\n",
        "            'id', 'issue_d', 'home_ownership_cat', 'income_category', 'income_cat', 'term_cat', 'application_type_cat',\n",
        "            'purpose_cat', 'interest_payment_cat', 'loan_condition'\n",
        "        ]\n",
        "        self.dataset.drop(columns=columns_to_delete, inplace=True)\n",
        "        self.dataset['grade'] = self.dataset['grade'].apply(temp_func_1)\n",
        "        self.dataset['final_d'] = self.dataset['final_d'].apply(lambda x: str(x)[-4:]).apply(temp_func_1)\n",
        "        self.dataset = pd.get_dummies(self.dataset, columns=['year', 'final_d', 'home_ownership', 'term', 'application_type',\n",
        "                                                             'purpose', 'interest_payments', 'grade', 'region'], dtype=int)\n",
        "\n",
        "    def train_test_split(self, test_size=0.2, random_state=42):\n",
        "        X = self.dataset.drop(columns=[self.label])\n",
        "        y = self.dataset[self.label]\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "        self.original_columns = X.columns\n",
        "\n",
        "        # Sort by index\n",
        "        self.X_train.sort_index(inplace=True)\n",
        "        self.X_train.reset_index(drop=True, inplace=True)\n",
        "        self.X_test.sort_index(inplace=True)\n",
        "        self.X_test.reset_index(drop=True, inplace=True)\n",
        "        self.y_train.sort_index(inplace=True)\n",
        "        self.y_train.reset_index(drop=True, inplace=True)\n",
        "        self.y_test.sort_index(inplace=True)\n",
        "        self.y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def preprocessing_train(self, exclude_columns=None):\n",
        "        if exclude_columns is None:\n",
        "            exclude_columns = []\n",
        "\n",
        "        # Separate columns to scale and exclude\n",
        "        columns_to_scale = [col for col in self.X_train.columns if col not in exclude_columns]\n",
        "\n",
        "        # Scale only the specified columns\n",
        "        scaler = StandardScaler()\n",
        "        self.X_train[columns_to_scale] = scaler.fit_transform(self.X_train[columns_to_scale])\n",
        "        self.scalers = scaler\n",
        "\n",
        "        # # Perform PCA\n",
        "        self.pca = PCA(n_components=30)\n",
        "        pca_components = self.pca.fit_transform(self.X_train)\n",
        "        self.X_train = pd.DataFrame(pca_components, columns=self.original_columns[:pca_components.shape[1]])\n",
        "\n",
        "    def preprocessing_test(self):\n",
        "        # Apply stored scalers\n",
        "        self.X_test = pd.DataFrame(self.scalers.transform(self.X_test), columns=self.original_columns)\n",
        "\n",
        "        # # Apply PCA\n",
        "        pca_components = self.pca.transform(self.X_test)\n",
        "        self.X_test = pd.DataFrame(pca_components, columns=self.original_columns[:pca_components.shape[1]])"
      ],
      "metadata": {
        "id": "lmLx5bbgBtcq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating # ok\n",
        "data = Dataset('/content/drive/My Drive/Colab Notebooks/FTEC4998_9/loan_final313_processed.feather')\n",
        "data.basic_processing()\n",
        "data.train_test_split()\n",
        "data.preprocessing_train()\n",
        "data.preprocessing_test()"
      ],
      "metadata": {
        "id": "13kLSUwPBvO9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data conversion # ok\n",
        "train_x, train_y = data.X_train, data.y_train\n",
        "test_x, test_y = data.X_test, data.y_test\n",
        "counts = np.mean(train_y == 1) * 100\n",
        "print(counts)\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(test_x.shape, test_y.shape)\n",
        "# Ensure y_train is binary\n",
        "assert set(train_y).issubset({0, 1}), \"Target values must be 0 or 1 for binary classification.\"\n",
        "# Move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EMPCAhUjB4tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03862c32-c05a-4f99-b71c-bd205403e018"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.5910370853482805\n",
            "(709903, 31) (709903,)\n",
            "(177476, 30) (177476,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x.shape)\n",
        "print(train_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgrMPSarnzsb",
        "outputId": "c5b904ef-9cd6-442d-fecd-49ccd528e2fa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(709903, 31)\n",
            "(709903,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, predict, and accuracy functions\n",
        "def train_model(model): # ok\n",
        "    print(\"Training.\")\n",
        "    model.train()\n",
        "    for epoch in range(model.epochs):\n",
        "        model.optimizer.zero_grad()\n",
        "        outputs = model(model.train_x)\n",
        "        loss = model.criterion(outputs, model.train_y)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "        if epoch % 5 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "def predict_model(model, X):\n",
        "    print('Predicting.')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X).squeeze()\n",
        "        return (outputs > 0.5).float()\n",
        "\n",
        "def calculate_accuracy(model, X, y):\n",
        "    print('Calculating Accuracy.')\n",
        "    X = X.to(next(model.parameters()).device)\n",
        "    y = y.to(next(model.parameters()).device)\n",
        "    predictions = predict_model(model, X)\n",
        "\n",
        "    # Ensure predictions and labels are the same shape\n",
        "    predictions = predictions.squeeze()\n",
        "    y = y.squeeze()\n",
        "\n",
        "    correct = (predictions == y).sum().item()\n",
        "    accuracy = correct / y.size(0)\n",
        "    return accuracy\n",
        "\n",
        "def df_to_tensor(x, y):\n",
        "    assert isinstance(x, pd.DataFrame) and isinstance(y, pd.Series)\n",
        "    return torch.tensor(x.to_numpy(), dtype=torch.float32).to(device), torch.tensor(y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "test_x, test_y = data.X_test, data.y_test\n",
        "test_x_tensor, test_y_tensor = df_to_tensor(data.X_test, data.y_test)"
      ],
      "metadata": {
        "id": "5UL36a0-Tq97"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP model\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, train_x, train_y, lr=0.001):\n",
        "        super(ANN, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.03),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 10\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "# Logistic Regression as a neural network\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, train_x, train_y, lr=0.01):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 700\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# SVM\n",
        "class SVMClassifier():\n",
        "    def __init__(self, train_x, train_y, fraction=0.1, n_samples=10000000000000):\n",
        "        # Sample a fraction of the data\n",
        "        n = min(int(len(train_x) * fraction), n_samples)\n",
        "        self.train_x = train_x.iloc[:n, :]\n",
        "        self.train_y = train_y.iloc[:n]\n",
        "        self.model = None\n",
        "    def fit(self):\n",
        "        print(\"Training.\")\n",
        "        # Use Bagging with SVM\n",
        "        self.model = BaggingClassifier(\n",
        "            estimator=SVC(C=0.1, kernel='poly', degree=5, gamma='scale'),\n",
        "            n_estimators=6,\n",
        "            random_state=42,\n",
        "            max_samples= 0.05\n",
        "        )\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        print('Predicting.')\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y):\n",
        "        print('Calculating Accuracy.')\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy\n",
        "\n",
        "# NB\n",
        "class NaiveBayesClassifier():\n",
        "    def __init__(self, train_x, train_y, priors=None, var_smoothing=1e-9):\n",
        "        self.model = GaussianNB(priors=priors, var_smoothing=var_smoothing)\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "\n",
        "    def fit(self):\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy\n",
        "# RF\n",
        "class RandomForestModel():\n",
        "    def __init__(self, train_x, train_y, n_estimators=10, max_depth=None, random_state=42):\n",
        "        self.model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state, max_samples = 0.05)\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "\n",
        "    def fit(self):\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "bqCIknAetfip"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class Adaboost():\n",
        "    def __init__(self, classes_dict, train_x, train_y):\n",
        "        self.classes_dict = classes_dict\n",
        "        self.model_order = list(classes_dict.keys())\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.sampled_train_x = None\n",
        "        self.sampled_train_y = None\n",
        "        self.trained_model = {}\n",
        "        self.training_data_history = {}\n",
        "        self.current_weight = None\n",
        "        self.restart = False\n",
        "    def weight_init(self):\n",
        "        self.current_weight = pd.Series(np.ones(len(self.train_y)))\n",
        "    def weight_calculate(self, current_prediction):\n",
        "        # Calculate error rate\n",
        "        error_rate = np.sum(self.current_weight * (1 - predictions)) / np.sum(self.current_weight)\n",
        "        if error_rate > 0.5: self.weight_init(); self.restart = True; return\n",
        "        alpha = 0.5 * np.log((1 - error_rate) / error_rate)\n",
        "        temp_weight = np.exp(alpha * (1 - predictions))\n",
        "        # temp_weight /= temp_weight.sum()\n",
        "        self.current_weight = temp_weight\n",
        "    def data_sampling(self, model):\n",
        "        self.train_x['weight'] = self.current_weight\n",
        "        self.train_y = self.train_y.to_frame()\n",
        "        self.train_y['weight'] = self.current_weight\n",
        "        self.sampled_train_x = self.train_x.sample(n=len(self.train_x), replace=True, weights='weight', random_state=42)\n",
        "        del self.sampled_train_x['weight']\n",
        "        self.sampled_train_x.sort_index(inplace=True)\n",
        "        self.sampled_train_x.reset_index(drop=True, inplace=True)\n",
        "        self.train_x = self.sampled_train_x\n",
        "        self.sampled_train_y = self.train_y.sample(n=len(self.train_y), replace=True, weights='weight', random_state=42)\n",
        "        del self.sampled_train_y['weight']\n",
        "        self.sampled_train_y.sort_index(inplace=True)\n",
        "        self.sampled_train_y.reset_index(drop=True, inplace=True)\n",
        "        self.sampled_train_y = self.sampled_train_y['loan_condition_cat']\n",
        "        self.train_y = self.sampled_train_y\n",
        "        self.training_data_history.update({model: {'x':self.train_x, 'y':self.train_y}})\n",
        "    def training(self):\n",
        "        self.weight_init()\n",
        "        for i in self.model_order:\n",
        "            while True:\n",
        "                self.restart = False\n",
        "                self.data_sampling(i)\n",
        "                print(\"*\" * 100)\n",
        "                print(f'Training --------------------- {i}')\n",
        "                current_model = self.classes_dict[i](self.sampled_train_x, self.sampled_train_y)\n",
        "                methods = inspect.getmembers(current_model, predicate=inspect.ismethod)\n",
        "                if 'fit' in [z for z, j in methods]:\n",
        "                    current_model.fit()\n",
        "                    current_prediction = current_model.predict(current_model.train_x)\n",
        "                    train_accuracy = current_model.calculate_accuracy(current_model.train_x, current_model.train_y)\n",
        "                else:\n",
        "                    current_model.to(device)\n",
        "                    train_model(current_model)\n",
        "                    current_prediction = predict_model(current_model, current_model.train_x).cpu().numpy()\n",
        "                    train_accuracy = calculate_accuracy(current_model, current_model.train_x, current_model.train_y)\n",
        "                print(f'{i} training accuracy:', train_accuracy)\n",
        "                print(\"-------->\", current_prediction)\n",
        "                print(\"-------->\", type(current_prediction))\n",
        "                print(\"-------->\", np.unique(current_prediction))\n",
        "                self.trained_model.update({i: current_model})\n",
        "                self.weight_calculate(current_prediction)\n",
        "                if self.restart == False: break\n",
        "\n",
        "# adModel = Adaboost(classes_dict={\"ANN\": ANN, \"LR\": LogisticRegressionModel, \"SVM\": SVMClassifier, \"NB\": NaiveBayesClassifier, \"RF\": RandomForestModel}, train_x=train_x, train_y=train_y)\n",
        "adModel = Adaboost(classes_dict={\"ANN\": ANN, \"LR\": LogisticRegressionModel, \"NB\": NaiveBayesClassifier}, train_x=train_x, train_y=train_y)\n",
        "# adModel.training()\n"
      ],
      "metadata": {
        "id": "fkw5jbXSIpfK",
        "collapsed": true
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ2vWXWUydAs",
        "outputId": "4e89aeff-0acb-4514-ad42-75ac01938f47"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ANN', 'LR', 'SVM', 'NB', 'RF']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkyIq7qE0MZk",
        "outputId": "ff5d9c64-c878-4fc0-9e3c-7fc76e2a4e31"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(709903, 30)\n",
            "(709903, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dsGPYfJhonZY"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K54rfOI6rKQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}