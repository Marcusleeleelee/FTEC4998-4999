{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM75gAtLov0GExBtKRRIOjP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcusleeleelee/FTEC4998-4999/blob/main/FTEC4998_4999.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Import the packages - ok\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from time import sleep\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9Irism69Bo6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Utils - ok\n",
        "def uni_list(input): return list(set(input))\n",
        "def perform_pca(df, n_components):\n",
        "\n",
        "    # Performing PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    principal_components = pca.fit_transform(df)\n",
        "\n",
        "    # Creating a DataFrame with the top 15 components\n",
        "    pca_df = pd.DataFrame(data=principal_components, index=df.index)\n",
        "\n",
        "    # Retaining the original column names for the new DataFrame\n",
        "    retained_columns = df.columns[:n_components]\n",
        "    pca_df.columns = retained_columns\n",
        "\n",
        "    return pca_df"
      ],
      "metadata": {
        "id": "ndu77M8kBr-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Dataset\n",
        "class Dataset:\n",
        "    def __init__(self, file_path): # ok\n",
        "        self.dataset = pd.read_feather(file_path)\n",
        "        self.train_dict, self.test_dict = {}, {}\n",
        "        self.scalers = None\n",
        "        self.pca = None\n",
        "        self.label = 'loan_condition_cat'\n",
        "\n",
        "    def show(self): # ok\n",
        "        return self.dataset.head(10)\n",
        "\n",
        "    def get(self, type, key): # ok\n",
        "        if type == 'test': return self.test_dict[key]['x'], self.test_dict[key]['y']\n",
        "        elif type == 'train': return self.train_dict[key]['x'], self.train_dict[key]['y']\n",
        "        else: raise Exception('The type must be either \"test\" or \"train\"!!')\n",
        "\n",
        "    def update(self, type, key, x, y):\n",
        "        if type == 'test': self.test_dict.update({key: {'x': x, 'y': y}})\n",
        "        elif type == 'train': self.train_dict.update({key: {'x': x, 'y': y}})\n",
        "        else: raise Exception('The type must be either \"test\" or \"train\"!!')\n",
        "\n",
        "    def basic_processing(self): # ok\n",
        "        temp_func_1 = lambda x: '<=2009' if str(x) in ['2007', '2008', '2009'] else (\"[2010, 2012]\" if str(x) in ['2010', '2011', '2012'] else '>=2013')\n",
        "        columns_to_delete = [\n",
        "            'id', 'issue_d', 'home_ownership_cat', 'income_category', 'income_cat', 'term_cat', 'application_type_cat',\n",
        "            'purpose_cat', 'interest_payment_cat', 'loan_condition'\n",
        "        ]\n",
        "        self.dataset.drop(columns=columns_to_delete, inplace=True)\n",
        "        self.dataset['grade'] = self.dataset['grade'].apply(temp_func_1)\n",
        "        self.dataset['final_d'] = self.dataset['final_d'].apply(lambda x: str(x)[-4:]).apply(temp_func_1)\n",
        "        self.dataset = pd.get_dummies(self.dataset, columns=['year', 'final_d', 'home_ownership', 'term', 'application_type',\n",
        "                                                             'purpose', 'interest_payments', 'grade', 'region'], dtype=int)\n",
        "        self.dataset.dropna(inplace=True)\n",
        "\n",
        "    def train_test_split(self, percentage=0.8): # ok\n",
        "        self.dataset = self.dataset.sample(frac=1).reset_index(drop=True)\n",
        "        train_size = int(len(self.dataset) * percentage)\n",
        "        temp1 = self.dataset.iloc[:train_size].copy()\n",
        "        temp2 = self.dataset.iloc[train_size:].copy()\n",
        "\n",
        "        y_train = temp1[[self.label]]\n",
        "        x_train = temp1.drop(columns=[self.label])\n",
        "        self.update('train', 'w0', x_train, y_train)\n",
        "\n",
        "        y_test = temp2[[self.label]]\n",
        "        x_test = temp2.drop(columns=[self.label])\n",
        "        self.update('test', 'w0', x_test, y_test)\n",
        "\n",
        "    def preprocessing_train(self):\n",
        "        temp_train_x, temp_train_y = self.get('train', 'w0')\n",
        "        scaler = StandardScaler()\n",
        "        temp_train_x = pd.DataFrame(scaler.fit_transform(temp_train_x), columns=temp_train_x.columns)\n",
        "        self.scalers = scaler\n",
        "        temp_train_x = perform_pca(temp_train_x, n_components=30)\n",
        "        self.update('train', 'w0', temp_train_x, temp_train_y)\n",
        "\n",
        "    def preprocessing_test(self): # Not ok\n",
        "        temp_test_x, temp_test_y = self.get('test', 'w0')\n",
        "        temp_train_x, _ = self.get('train', 'w0')\n",
        "        print(temp_train_x)\n",
        "\n",
        "        # Apply stored scalers\n",
        "        temp_test_x = pd.DataFrame(self.scalers.transform(temp_test_x), columns=temp_train_x.columns)\n",
        "\n",
        "        assert set(temp_test_x.columns) == set(temp_train_x.columns)\n",
        "        self.update('test', 'w0', temp_test_x, temp_test_y)\n",
        "\n",
        "    def resample_with_weights(self, model, weight): # Not ok\n",
        "        temp_train_x, temp_train_y = self.get('train', weight)\n",
        "        y_pred = model.predict(temp_train_x)\n",
        "\n",
        "        misclassified = (temp_train_y[self.label].values != y_pred)\n",
        "        weights = np.ones(len(temp_train_y))\n",
        "\n",
        "        if misclassified.any():\n",
        "            weights[misclassified] = 1.0 / misclassified.sum()\n",
        "        if (~misclassified).any():\n",
        "            weights[~misclassified] = 1.0 / (~misclassified).sum()\n",
        "\n",
        "        weights /= weights.sum()\n",
        "\n",
        "        sampled_indices = np.random.choice(temp_train_x.index, size=len(temp_train_x), replace=True, p=weights)\n",
        "        temp_x = temp_train_x.loc[sampled_indices]\n",
        "        temp_y = temp_train_y.loc[sampled_indices]\n",
        "        self.update('train', 'w' + str(int(weight[1:]) + 1), temp_x, temp_y)"
      ],
      "metadata": {
        "id": "lmLx5bbgBtcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating\n",
        "data = Dataset('/content/drive/My Drive/Colab Notebooks/FTEC4998_9/loan_final313_processed.feather')\n",
        "data.basic_processing()\n",
        "data.train_test_split()\n",
        "data.preprocessing_train()"
      ],
      "metadata": {
        "id": "13kLSUwPBvO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "train_x, train_y = data.get('train', 'w0')\n",
        "train_y = train_y.values.ravel()\n",
        "counts = np.mean(train_y == 1) * 100\n",
        "print(counts)\n",
        "# Convert to NumPy arrays\n",
        "X_train = train_x.to_numpy()\n",
        "y_train = train_y\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "EMPCAhUjB4tJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Modelling Training\n",
        "# Check target values\n",
        "assert set(y_train).issubset({0, 1}), \"Target values must be 0 or 1 for binary classification.\"\n",
        "\n",
        "# Set CUDA launch blocking for debugging\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "# Convert to tensors and move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "try:\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
        "except RuntimeError as e: print(\"Tensor conversion error:\", e); raise\n",
        "\n",
        "# Logistic Regression as a neural network\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "# MLP model\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Train and predict function for models\n",
        "def train_model(model, criterion, optimizer, X_train, y_train, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train).squeeze()\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def predict_model(model, X):\n",
        "    model.eval()\n",
        "    with torch.no_grad(): outputs = model(X).squeeze()\n",
        "    return (outputs > 0.5).float()\n",
        "\n",
        "# Initialize PyTorch models and move to GPU\n",
        "log_reg_model = LogisticRegressionModel(X_train_tensor.shape[1]).to(device)\n",
        "mlp_model = MLPModel(X_train_tensor.shape[1]).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_log_reg = optim.SGD(log_reg_model.parameters(), lr=0.01)\n",
        "optimizer_mlp = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train PyTorch models\n",
        "train_model(log_reg_model, criterion, optimizer_log_reg, X_train_tensor, y_train_tensor)\n",
        "train_model(mlp_model, criterion, optimizer_mlp, X_train_tensor, y_train_tensor)\n",
        "\n",
        "# Predict with PyTorch models and move predictions to CPU\n",
        "log_reg_predictions = predict_model(log_reg_model, X_train_tensor).cpu().numpy()\n",
        "mlp_predictions = predict_model(mlp_model, X_train_tensor).cpu().numpy()\n",
        "\n",
        "# Scikit-learn models\n",
        "svm = SVC(probability=True)\n",
        "naive_bayes = GaussianNB()\n",
        "random_forest = RandomForestClassifier()\n",
        "\n",
        "# Train Scikit-learn models\n",
        "svm.fit(X_train, y_train)\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# Predict with Scikit-learn models\n",
        "svm_predictions = svm.predict(X_train)\n",
        "naive_bayes_predictions = naive_bayes.predict(X_train)\n",
        "rf_predictions = random_forest.predict(X_train)\n",
        "\n",
        "# Collect predictions for stacking\n",
        "predictions = {\n",
        "    \"log_reg\": log_reg_predictions,\n",
        "    \"mlp\": mlp_predictions,\n",
        "    \"svm\": svm_predictions,\n",
        "    \"naive_bayes\": naive_bayes_predictions,\n",
        "    \"random_forest\": rf_predictions,\n",
        "}\n",
        "\n",
        "# Stack predictions as features for AdaBoost\n",
        "stacked_features = np.column_stack(list(predictions.values()))\n",
        "\n",
        "# Initialize and train AdaBoost\n",
        "ada_boost = AdaBoostClassifier(n_estimators=10, random_state=42)\n",
        "ada_boost.fit(stacked_features, y_train)\n",
        "\n",
        "# Evaluate\n",
        "stacked_accuracy = ada_boost.score(stacked_features, y_train)\n",
        "print(\"Stacked model accuracy:\", stacked_accuracy)"
      ],
      "metadata": {
        "id": "ihWK4dogCDQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQxTOBOkHlsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9YtaBSy9ZyUO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}