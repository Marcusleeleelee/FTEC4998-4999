{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGFq+HO+L/+LRUaitNx0XK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcusleeleelee/FTEC4998-4999/blob/main/FTEC4998_4999.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import inspect\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9Irism69Bo6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86575a2-b083-49b3-9e5b-95c1809a1f6c"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Utils - ok\n",
        "def uni_list(input): return list(set(input))"
      ],
      "metadata": {
        "id": "ndu77M8kBr-h"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset():\n",
        "    def __init__(self, file_path):\n",
        "        self.dataset = pd.read_feather(file_path)\n",
        "        self.X_train, self.y_train = None, None\n",
        "        self.X_test, self.y_test = None, None\n",
        "        self.scalers = None\n",
        "        self.pca = None\n",
        "        self.label = 'loan_condition_cat'\n",
        "        self.original_columns = None\n",
        "\n",
        "    def show(self, rows=10):\n",
        "        return self.dataset.head(rows)\n",
        "\n",
        "    def basic_processing(self):\n",
        "        temp_func_1 = lambda x: '<=2009' if str(x) in ['2007', '2008', '2009'] else (\"[2010, 2012]\" if str(x) in ['2010', '2011', '2012'] else '>=2013')\n",
        "        columns_to_delete = [\n",
        "            'id', 'issue_d', 'home_ownership_cat', 'income_category', 'income_cat', 'term_cat', 'application_type_cat',\n",
        "            'purpose_cat', 'interest_payment_cat', 'loan_condition'\n",
        "        ]\n",
        "        self.dataset.drop(columns=columns_to_delete, inplace=True)\n",
        "        self.dataset['grade'] = self.dataset['grade'].apply(temp_func_1)\n",
        "        self.dataset['final_d'] = self.dataset['final_d'].apply(lambda x: str(x)[-4:]).apply(temp_func_1)\n",
        "        self.dataset = pd.get_dummies(self.dataset, columns=['year', 'final_d', 'home_ownership', 'term', 'application_type',\n",
        "                                                             'purpose', 'interest_payments', 'grade', 'region'], dtype=int)\n",
        "\n",
        "    def train_test_split(self, test_size=0.2, random_state=42):\n",
        "        X = self.dataset.drop(columns=[self.label])\n",
        "        y = self.dataset[self.label]\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "        self.original_columns = X.columns\n",
        "\n",
        "        # Sort by index\n",
        "        self.X_train.sort_index(inplace=True)\n",
        "        self.X_test.sort_index(inplace=True)\n",
        "        self.y_train.sort_index(inplace=True)\n",
        "        self.y_test.sort_index(inplace=True)\n",
        "\n",
        "    def preprocessing_train(self, exclude_columns=None):\n",
        "        if exclude_columns is None:\n",
        "            exclude_columns = []\n",
        "\n",
        "        # Separate columns to scale and exclude\n",
        "        columns_to_scale = [col for col in self.X_train.columns if col not in exclude_columns]\n",
        "\n",
        "        # Scale only the specified columns\n",
        "        scaler = StandardScaler()\n",
        "        self.X_train[columns_to_scale] = scaler.fit_transform(self.X_train[columns_to_scale])\n",
        "        self.scalers = scaler\n",
        "\n",
        "        # # Perform PCA\n",
        "        self.pca = PCA(n_components=30)\n",
        "        pca_components = self.pca.fit_transform(self.X_train)\n",
        "        self.X_train = pd.DataFrame(pca_components, columns=self.original_columns[:pca_components.shape[1]])\n",
        "\n",
        "    def preprocessing_test(self):\n",
        "        # Apply stored scalers\n",
        "        self.X_test = pd.DataFrame(self.scalers.transform(self.X_test), columns=self.original_columns)\n",
        "\n",
        "        # # Apply PCA\n",
        "        pca_components = self.pca.transform(self.X_test)\n",
        "        self.X_test = pd.DataFrame(pca_components, columns=self.original_columns[:pca_components.shape[1]])"
      ],
      "metadata": {
        "id": "lmLx5bbgBtcq"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating # ok\n",
        "data = Dataset('/content/drive/My Drive/Colab Notebooks/FTEC4998_9/loan_final313_processed.feather')\n",
        "data.basic_processing()\n",
        "data.train_test_split()\n",
        "data.preprocessing_train()\n",
        "data.preprocessing_test()"
      ],
      "metadata": {
        "id": "13kLSUwPBvO9"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data conversion # ok\n",
        "train_x, train_y = data.X_train, data.y_train\n",
        "test_x, test_y = data.X_test, data.y_test\n",
        "counts = np.mean(train_y == 1) * 100\n",
        "print(counts)\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(test_x.shape, test_y.shape)\n",
        "# Ensure y_train is binary\n",
        "assert set(train_y).issubset({0, 1}), \"Target values must be 0 or 1 for binary classification.\"\n",
        "# Move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "EMPCAhUjB4tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4961dc-c18e-4878-ccdb-2681f8fad4b5"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.5910370853482805\n",
            "(709903, 30) (709903,)\n",
            "(177476, 54) (177476,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x.shape)\n",
        "print(train_y.shape)"
      ],
      "metadata": {
        "id": "FgrMPSarnzsb",
        "outputId": "7f9234e4-6f8d-4a94-ba85-0e464ca24a05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(709903, 31)\n",
            "(709904,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train, predict, and accuracy functions\n",
        "def train_model(model): # ok\n",
        "    print(\"Training.\")\n",
        "    model.train()\n",
        "    for epoch in range(model.epochs):\n",
        "        model.optimizer.zero_grad()\n",
        "        outputs = model(model.train_x)\n",
        "        loss = model.criterion(outputs, model.train_y)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "        if epoch % 5 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "def predict_model(model, X):\n",
        "    print('Predicting.')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X).squeeze()\n",
        "        return (outputs > 0.5).float()\n",
        "\n",
        "def calculate_accuracy(model, X, y):\n",
        "    print('Calculating Accuracy.')\n",
        "    X = X.to(next(model.parameters()).device)\n",
        "    y = y.to(next(model.parameters()).device)\n",
        "    predictions = predict_model(model, X)\n",
        "\n",
        "    # Ensure predictions and labels are the same shape\n",
        "    predictions = predictions.squeeze()\n",
        "    y = y.squeeze()\n",
        "\n",
        "    correct = (predictions == y).sum().item()\n",
        "    accuracy = correct / y.size(0)\n",
        "    return accuracy\n",
        "\n",
        "def df_to_tensor(x, y):\n",
        "    assert isinstance(x, pd.DataFrame) and isinstance(y, pd.Series)\n",
        "    return torch.tensor(x.to_numpy(), dtype=torch.float32).to(device), torch.tensor(y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "test_x, test_y = data.X_test, data.y_test\n",
        "test_x_tensor, test_y_tensor = df_to_tensor(data.X_test, data.y_test)"
      ],
      "metadata": {
        "id": "5UL36a0-Tq97"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP model\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, train_x, train_y, lr=0.001):\n",
        "        super(ANN, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.LeakyReLU(0.03),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 10\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "# Logistic Regression as a neural network\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, train_x, train_y, lr=0.01):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 700\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# SVM\n",
        "class SVMClassifier():\n",
        "    def __init__(self, train_x, train_y, fraction=0.1, n_samples=10000000000000):\n",
        "        # Sample a fraction of the data\n",
        "        n = min(int(len(train_x) * fraction), n_samples)\n",
        "        self.train_x = train_x.iloc[:n, :]\n",
        "        self.train_y = train_y.iloc[:n]\n",
        "        self.model = None\n",
        "    def fit(self):\n",
        "        print(\"Training.\")\n",
        "        # Use Bagging with SVM\n",
        "        self.model = BaggingClassifier(\n",
        "            estimator=SVC(C=0.1, kernel='poly', degree=5, gamma='scale'),\n",
        "            n_estimators=6,\n",
        "            random_state=42,\n",
        "            max_samples= 0.05\n",
        "        )\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        print('Predicting.')\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y):\n",
        "        print('Calculating Accuracy.')\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy\n",
        "\n",
        "# NB\n",
        "class NaiveBayesClassifier():\n",
        "    def __init__(self, train_x, train_y, priors=None, var_smoothing=1e-9):\n",
        "        self.model = GaussianNB(priors=priors, var_smoothing=var_smoothing)\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "\n",
        "    def fit(self):\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy\n",
        "# RF\n",
        "class RandomForestModel():\n",
        "    def __init__(self, train_x, train_y, n_estimators=10, max_depth=None, random_state=42):\n",
        "        self.model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state, max_samples = 0.05)\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "\n",
        "    def fit(self):\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "bqCIknAetfip"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class Adaboost():\n",
        "    def __init__(self, classes_dict, train_x, train_y):\n",
        "        self.classes_dict = classes_dict\n",
        "        self.model_order = list(classes_dict.keys())\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.sampled_train_x = None\n",
        "        self.sampled_train_y = None\n",
        "        self.current_weight = -1\n",
        "        self.trained_model = {}\n",
        "    def weight_calculate(self, current_prediction): pass\n",
        "    def data_sampling(self):\n",
        "        if self.current_weight == -1:\n",
        "            self.train_x['weight'] = 1\n",
        "            self.train_y = self.train_y.to_frame()\n",
        "            self.train_y['weight'] = 1\n",
        "        else: pass\n",
        "        return\n",
        "        self.sampled_train_x = self.train_x.sample(n=len(self.train_x), replace=True, weights='weight', random_state=42)\n",
        "        del self.sampled_train_x['weight']\n",
        "        self.sampled_train_x.reset_index(drop=True, inplace=True)\n",
        "        self.train_x = self.sampled_train_x\n",
        "        self.sampled_train_y = self.train_y.sample(n=len(self.train_y), replace=True, weights='weight', random_state=42)\n",
        "        del self.sampled_train_y['weight']\n",
        "        self.sampled_train_y.reset_index(drop=True, inplace=True)\n",
        "        self.sampled_train_y = self.sampled_train_y['loan_condition_cat']\n",
        "        self.train_y = self.sampled_train_y\n",
        "    def training(self):\n",
        "        for i in self.model_order:\n",
        "            self.weight_sampling()\n",
        "            print(\"*\" * 100)\n",
        "            print(f'Training --- {i}')\n",
        "            current_model = self.classes_dict[i](self.sampled_train_x, self.sampled_train_y)\n",
        "            methods = inspect.getmembers(current_model, predicate=inspect.ismethod)\n",
        "            if 'fit' in [i for i, j in methods]:\n",
        "                current_model.fit()\n",
        "                current_prediction = current_model.predict(current_model.train_x)\n",
        "                train_accuracy = current_model.calculate_accuracy(current_model.train_x, current_model.train_y)\n",
        "            else:\n",
        "                current_model.to(device)\n",
        "                train_model(current_model)\n",
        "                current_prediction = predict_model(current_model, current_model.train_x).cpu().numpy()\n",
        "                train_accuracy = calculate_accuracy(current_model, current_model.train_x, current_model.train_y)\n",
        "            print(f'{i} training accuracy:', train_accuracy)\n",
        "            print(\"-------->\", current_prediction)\n",
        "            print(\"-------->\", type(current_prediction))\n",
        "            print(np.unique(\"-------->\", current_prediction))\n",
        "            self.trained_model.update({i: current_model})\n",
        "            self.weight_calculate(current_prediction)\n",
        "\n",
        "adModel = Adaboost(classes_dict={\"ANN\": ANN, \"LR\": LogisticRegressionModel, \"SVM\": SVMClassifier, \"NB\": NaiveBayesClassifier, \"RF\": RandomForestModel}, train_x=train_x, train_y=train_y)\n",
        "# adModel.training()\n"
      ],
      "metadata": {
        "id": "fkw5jbXSIpfK",
        "collapsed": true
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(adModel.model_order)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ2vWXWUydAs",
        "outputId": "3e95c7fd-293e-408a-a5d4-e4273dad4fa3"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ANN', 'LR', 'SVM', 'NB', 'RF']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(adModel.train_y.shape)\n",
        "print(adModel.train_x.shape)"
      ],
      "metadata": {
        "id": "GkyIq7qE0MZk",
        "outputId": "f505df44-3327-4c1a-c524-a39015bcebf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(709904,)\n",
            "(709903, 31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(adModel.train_x.head())"
      ],
      "metadata": {
        "id": "tcoy_JYn0cCD",
        "outputId": "1056e059-6555-44b2-c49e-beb71b2ca14a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emp_length_int  annual_inc  loan_amount  interest_rate  grade_cat  \\\n",
            "0        0.442117    2.941908    -2.110152      -0.202310  -0.290872   \n",
            "1       -0.082109    2.081844    -2.536040      -1.141946   0.045146   \n",
            "2        0.287807    0.738503    -2.355604      -1.908104   0.364486   \n",
            "3       -1.165200    1.749026    -1.681660      -0.424047  -0.012104   \n",
            "4       -3.034633    0.032442    -2.915756      -1.169525   0.300317   \n",
            "\n",
            "        dti  total_pymnt  total_rec_prncp  recoveries  installment  ...  \\\n",
            "0 -0.883359    -1.270727        -3.034184    0.978829     0.638625  ...   \n",
            "1 -1.419896    -1.085570        -1.339121   -0.965590     0.603005  ...   \n",
            "2 -0.277135    -1.233532        -1.402966   -1.171648     1.061614  ...   \n",
            "3  0.472921    -1.651971        -2.403974    0.590908     1.790541  ...   \n",
            "4 -0.074204    -0.939012        -3.118483   -0.845813    -0.235582  ...   \n",
            "\n",
            "   final_d_[2010, 2012]  home_ownership_ANY  home_ownership_MORTGAGE  \\\n",
            "0             -0.936057           -0.963372                -0.272121   \n",
            "1              0.113417           -1.073041                 1.260188   \n",
            "2             -0.967700            0.015123                 0.287764   \n",
            "3             -1.655068            0.543982                 0.073917   \n",
            "4              5.329326            4.483543                -0.243359   \n",
            "\n",
            "   home_ownership_NONE  home_ownership_OTHER  home_ownership_OWN  \\\n",
            "0            -0.547901              2.274366           -1.274483   \n",
            "1             2.716996             -0.043367            0.147457   \n",
            "2             0.647666             -0.376716            0.507854   \n",
            "3             0.471108             -0.567072            0.464748   \n",
            "4             0.833465             -4.703909            2.379199   \n",
            "\n",
            "   home_ownership_RENT  term_ 36 months  term_ 60 months  weight  \n",
            "0             4.218386         1.262413        -1.947315       1  \n",
            "1            -0.146770        -1.312880         3.562066       1  \n",
            "2            -2.053364         0.174281         0.232669       1  \n",
            "3            -2.128030        -0.354662        -0.282819       1  \n",
            "4            -5.963605        -6.940273        -5.979868       1  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(adModel.train_y)"
      ],
      "metadata": {
        "id": "VxuLkAhAzC-i",
        "outputId": "1cd0dbb2-18af-4127-b6aa-71f55f5ab409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1         1\n",
            "2         0\n",
            "3         0\n",
            "4         0\n",
            "5         0\n",
            "         ..\n",
            "887374    0\n",
            "887375    0\n",
            "887377    0\n",
            "887378    0\n",
            "weight    1\n",
            "Name: loan_condition_cat, Length: 709904, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adModel.data_sampling()"
      ],
      "metadata": {
        "id": "BPfnpZA_WFHW",
        "outputId": "5626ee67-2ca5-468e-91ea-5d53a40a4a9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        }
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Strings cannot be passed as weights when sampling from a Series.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-186-9180d371ab1e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-184-a9b643d1a9a7>\u001b[0m in \u001b[0;36mdata_sampling\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_train_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_train_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_train_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_train_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_train_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, n, frac, replace, weights, random_state, axis, ignore_index)\u001b[0m\n\u001b[1;32m   6025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6027\u001b[0;31m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6029\u001b[0m         \u001b[0msampled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/sample.py\u001b[0m in \u001b[0;36mpreprocess_weights\u001b[0;34m(obj, weights, axis)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 )\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0;34m\"Strings cannot be passed as weights when sampling from a Series.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: Strings cannot be passed as weights when sampling from a Series."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame with uneven weights\n",
        "df = pd.DataFrame({\n",
        "    'A': [1, 2, 3, 4, 5],\n",
        "    'B': [6, 7, 8, 9, 10],\n",
        "    'weight': [3, 1.1, 0.7, 0.3, 0.1]  # Uneven weights\n",
        "})\n",
        "\n",
        "# Sample with replacement based on normalized weights\n",
        "sampled_df = df.sample(n=len(df) + 10, replace=True, weights='weight', random_state=311)\n",
        "\n",
        "print(sampled_df)"
      ],
      "metadata": {
        "id": "dsGPYfJhonZY",
        "outputId": "17483c66-e23b-4ac2-9831-93ec1a1eb7a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   A  B  weight\n",
            "0  1  6     3.0\n",
            "0  1  6     3.0\n",
            "1  2  7     1.1\n",
            "0  1  6     3.0\n",
            "0  1  6     3.0\n",
            "1  2  7     1.1\n",
            "0  1  6     3.0\n",
            "0  1  6     3.0\n",
            "2  3  8     0.7\n",
            "0  1  6     3.0\n",
            "0  1  6     3.0\n",
            "1  2  7     1.1\n",
            "0  1  6     3.0\n",
            "0  1  6     3.0\n",
            "2  3  8     0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K54rfOI6rKQ-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}