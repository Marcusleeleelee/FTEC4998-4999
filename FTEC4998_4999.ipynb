{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcusleeleelee/FTEC4998-4999/blob/main/FTEC4998_4999.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Irism69Bo6L",
        "outputId": "6f55c71b-558e-4b53-86e9-c6c2143145ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Import necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from xgboost import XGBClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n",
        "import inspect\n",
        "import matplotlib.pyplot as plt\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ndu77M8kBr-h"
      },
      "outputs": [],
      "source": [
        "# Step 1: Utils - ok\n",
        "def uni_list(input): return list(set(input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lmLx5bbgBtcq"
      },
      "outputs": [],
      "source": [
        "class Dataset():\n",
        "    def __init__(self, file_path):\n",
        "        self.dataset = pd.read_feather(file_path)\n",
        "        self.original = self.dataset.copy()\n",
        "        self.X_train, self.y_train = None, None\n",
        "        self.X_test, self.y_test = None, None\n",
        "        self.label = 'loan_condition_cat'\n",
        "        self.min_max_columns = ['annual_inc', 'year']\n",
        "        self.means = {}\n",
        "        self.stds = {}\n",
        "        self.mins = {}\n",
        "        self.maxs = {}\n",
        "\n",
        "    def show(self, rows=10):\n",
        "        return self.dataset.head(rows)\n",
        "\n",
        "    def basic_processing(self):\n",
        "        temp_func_2 = lambda x: {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G': 7}[str(x)]\n",
        "        columns_to_delete = [\n",
        "            'id', 'issue_d', 'home_ownership_cat', 'income_category', 'income_cat', 'term_cat', 'application_type_cat',\n",
        "            'purpose_cat', 'interest_payment_cat', 'loan_condition'\n",
        "        ]\n",
        "        self.dataset.drop(columns=columns_to_delete, inplace=True)\n",
        "        self.dataset['grade'] = self.dataset['grade'].apply(temp_func_2)\n",
        "        self.dataset['final_d'] = self.dataset['final_d'].apply(lambda x: str(x)[-4:]).apply(int)\n",
        "        self.dataset['year'] = self.dataset['year'].apply(lambda x: str(x)[-4:]).apply(int)\n",
        "        self.dataset = pd.get_dummies(self.dataset, columns=['home_ownership', 'term', 'application_type',\n",
        "                                                             'purpose', 'interest_payments', 'region'], dtype=int)\n",
        "\n",
        "\n",
        "    def train_test_split(self, test_size=0.2, random_state=42):\n",
        "        X = self.dataset.drop(columns=[self.label])\n",
        "        y = self.dataset[self.label]\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "        self.original_columns = X.columns\n",
        "\n",
        "        # Sort by index\n",
        "        self.X_train.sort_index(inplace=True)\n",
        "        self.X_train.reset_index(drop=True, inplace=True)\n",
        "        self.X_test.sort_index(inplace=True)\n",
        "        self.X_test.reset_index(drop=True, inplace=True)\n",
        "        self.y_train.sort_index(inplace=True)\n",
        "        self.y_train.reset_index(drop=True, inplace=True)\n",
        "        self.y_test.sort_index(inplace=True)\n",
        "        self.y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    def preprocessing_train(self):\n",
        "        # Separate columns for Min-Max and Z-score normalization\n",
        "        columns_to_normalize = self.min_max_columns\n",
        "        columns_to_scale = [col for col in self.X_train.columns if col not in columns_to_normalize]\n",
        "\n",
        "        # Z-score normalization\n",
        "        for col in columns_to_scale:\n",
        "            mean = np.mean(self.X_train[col])\n",
        "            std = np.std(self.X_train[col])\n",
        "            self.means[col] = mean\n",
        "            self.stds[col] = std\n",
        "            self.X_train[col] = (self.X_train[col] - mean) / std\n",
        "\n",
        "        # Min-Max normalization\n",
        "        for col in columns_to_normalize:\n",
        "            min_val = np.min(self.X_train[col])\n",
        "            max_val = np.max(self.X_train[col])\n",
        "            self.mins[col] = min_val\n",
        "            self.maxs[col] = max_val\n",
        "            self.X_train[col] = (self.X_train[col] - min_val) / (max_val - min_val)\n",
        "\n",
        "        # Perform PCA\n",
        "        selected_columns = self.perform_pca(self.X_train, n_components=35)\n",
        "        self.X_train = self.X_train[selected_columns]\n",
        "\n",
        "\n",
        "    def perform_pca(self, data, n_components):\n",
        "        # Center the data\n",
        "        data_mean = np.mean(data, axis=0)\n",
        "        centered_data = data - data_mean\n",
        "\n",
        "        # Compute covariance matrix\n",
        "        cov_matrix = np.cov(centered_data, rowvar=False)\n",
        "\n",
        "        # Eigen decomposition\n",
        "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
        "\n",
        "        # Sort eigenvectors by eigenvalues in descending order\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "        # Select the top n_components\n",
        "        selected_eigenvectors = sorted_eigenvectors[:, :n_components]\n",
        "\n",
        "        # Identify important features\n",
        "        feature_importance = np.abs(selected_eigenvectors).sum(axis=1)\n",
        "        important_indices = np.argsort(feature_importance)[::-1][:n_components]\n",
        "\n",
        "        # Return the original column names of these features\n",
        "        important_features = [self.original_columns[i] for i in important_indices]\n",
        "\n",
        "        return important_features\n",
        "\n",
        "    def preprocessing_test(self):\n",
        "        # Separate columns for Min-Max and Z-score normalization\n",
        "        columns_to_normalize = self.min_max_columns\n",
        "        columns_to_scale = [col for col in self.X_test.columns if col not in columns_to_normalize]\n",
        "\n",
        "        # Apply Z-score normalization using training statistics\n",
        "        for col in columns_to_scale:\n",
        "            self.X_test[col] = (self.X_test[col] - self.means[col]) / self.stds[col]\n",
        "\n",
        "        # Apply Min-Max normalization using training statistics\n",
        "        for col in columns_to_normalize:\n",
        "            self.X_test[col] = (self.X_test[col] - self.mins[col]) / (self.maxs[col] - self.mins[col])\n",
        "\n",
        "        # Apply PCA using training components\n",
        "        self.X_test = self.X_test[[i for i in self.X_train.columns.to_list()]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "13kLSUwPBvO9"
      },
      "outputs": [],
      "source": [
        "# Calculating # ok\n",
        "data = Dataset('/content/drive/My Drive/Colab Notebooks/FTEC4998_9/loan_final313_processed.feather')\n",
        "data.basic_processing()\n",
        "data.train_test_split()\n",
        "data.preprocessing_train()\n",
        "data.preprocessing_test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMPCAhUjB4tJ",
        "outputId": "9af404c3-884b-4576-a9ed-8925e9e15824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.5910370853482805\n",
            "(709903, 35) (709903,)\n",
            "(177476, 35) (177476,)\n"
          ]
        }
      ],
      "source": [
        "# Data conversion # ok\n",
        "train_x, train_y = data.X_train, data.y_train\n",
        "test_x, test_y = data.X_test, data.y_test\n",
        "counts = np.mean(train_y == 1) * 100\n",
        "print(counts)\n",
        "print(train_x.shape, train_y.shape)\n",
        "print(test_x.shape, test_y.shape)\n",
        "# Ensure y_train is binary\n",
        "assert set(train_y).issubset({0, 1}), \"Target values must be 0 or 1 for binary classification.\"\n",
        "# Move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5UL36a0-Tq97"
      },
      "outputs": [],
      "source": [
        "# Train, predict, and accuracy functions\n",
        "def train_model_pt(model): # ok\n",
        "    model.train()\n",
        "    for epoch in range(model.epochs):\n",
        "        model.optimizer.zero_grad()\n",
        "        outputs = model(model.train_x)\n",
        "        loss = model.criterion(outputs, model.train_y)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "        if epoch % 5 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "def predict_model_pt(model, X):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X = X.to(next(model.parameters()).device)\n",
        "        outputs = model(X).squeeze()\n",
        "        return (outputs > 0.5).float().cpu().numpy()  # Convert to numpy array\n",
        "def calculate_accuracy_pt(model, X, y, pred=None):\n",
        "    # Ensure X and y are on the correct device\n",
        "    X = X.to(next(model.parameters()).device)\n",
        "    y = y.to(next(model.parameters()).device)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = predict_model_pt(model, X) if pred is None else pred\n",
        "\n",
        "    # Ensure predictions and labels are tensors and the same shape\n",
        "    predictions = predictions.squeeze()\n",
        "    y = y.squeeze()\n",
        "\n",
        "    # Convert to tensors if necessary\n",
        "    if not isinstance(predictions, torch.Tensor):\n",
        "        predictions = torch.tensor(predictions)\n",
        "    if not isinstance(y, torch.Tensor):\n",
        "        y = torch.tensor(y)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    correct = (predictions == y).sum().item()\n",
        "    accuracy = correct / len(y)\n",
        "    return accuracy\n",
        "\n",
        "def df_to_tensor(x, y):\n",
        "    assert isinstance(x, pd.DataFrame) and isinstance(y, pd.Series)\n",
        "    return torch.tensor(x.to_numpy(), dtype=torch.float32).to(device), torch.tensor(y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "test_x, test_y = data.X_test, data.y_test\n",
        "test_x_tensor, test_y_tensor = df_to_tensor(data.X_test, data.y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bqCIknAetfip"
      },
      "outputs": [],
      "source": [
        "# MLP model\n",
        "class ANN(nn.Module): # Not yet finish the plot function\n",
        "    def __init__(self, train_x, train_y, lr=0.001):\n",
        "        super(ANN, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self._initialize_weights()\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 50\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.net:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "\n",
        "# Logistic Regression as a neural network\n",
        "class LogisticRegressionModel(nn.Module): # Not yet finish the plot function\n",
        "    def __init__(self, train_x, train_y, lr=0.001):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.train_y = torch.tensor(train_y.values.ravel(), dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        self.train_x = torch.tensor(train_x.to_numpy(), dtype=torch.float32).to(device)\n",
        "        self.input_dim = self.train_x.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 1200\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# SVM\n",
        "class SVMClassifier():\n",
        "    def __init__(self, train_x, train_y, fraction=0.1):\n",
        "        # Sample a fraction of the data\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.model = None\n",
        "        self.accuracies = []\n",
        "\n",
        "    def fit(self):\n",
        "        # Use Bagging with SVM\n",
        "        self.model = BaggingClassifier(\n",
        "            estimator=SVC(C=0.1, kernel='poly', degree=3, gamma='scale'),\n",
        "            n_estimators=6,\n",
        "            random_state=42,\n",
        "            max_samples=0.001\n",
        "        )\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "        predictions = self.model.predict(self.train_x)\n",
        "        accuracy = accuracy_score(self.train_y, predictions)\n",
        "        self.accuracies.append(accuracy)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o', linestyle='-')\n",
        "        plt.title('SVM Training Accuracy Curve')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "# NB\n",
        "class NaiveBayesClassifier():\n",
        "    def __init__(self, train_x, train_y, priors=None, var_smoothing=1e-9):\n",
        "        self.model = GaussianNB(priors=priors, var_smoothing=var_smoothing)\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.accuracies = []\n",
        "\n",
        "    def fit(self):\n",
        "        self.model.fit(self.train_x, self.train_y)\n",
        "        predictions = self.model.predict(self.train_x)\n",
        "        accuracy = accuracy_score(self.train_y, predictions)\n",
        "        self.accuracies.append(accuracy)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o', linestyle='-')\n",
        "        plt.title('Naive Bayes Training Accuracy Curve')\n",
        "        plt.xlabel('Iteration')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "\n",
        "# RF\n",
        "class RandomForestModel():\n",
        "    def __init__(self, train_x, train_y, n_estimators=10, max_depth=None, random_state=42):\n",
        "        self.model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state, max_samples=0.05, warm_start=True)\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.prediction = None\n",
        "        self.accuracies = []\n",
        "\n",
        "    def fit(self):\n",
        "        for i in range(1, self.model.n_estimators + 1):\n",
        "            self.model.n_estimators = i\n",
        "            self.model.fit(self.train_x, self.train_y)\n",
        "            predictions = self.model.predict(self.train_x)\n",
        "            accuracy = accuracy_score(self.train_y, predictions)\n",
        "            self.accuracies.append(accuracy)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(range(1, len(self.accuracies) + 1), self.accuracies, marker='o', linestyle='-')\n",
        "        plt.title('Random Forest Training Accuracy Curve')\n",
        "        plt.xlabel('Number of Trees')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid()\n",
        "        plt.show()\n",
        "#XBG\n",
        "class XGBoostClassifier():\n",
        "    def __init__(self, train_x, train_y, n_estimators=100,\n",
        "                 learning_rate=0.1, max_depth=6, subsample=0.8,\n",
        "                 colsample_bytree=0.8, gamma=0, reg_alpha=0, reg_lambda=1):\n",
        "        self.model = XGBClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=learning_rate,\n",
        "            max_depth=max_depth,\n",
        "            subsample=subsample,\n",
        "            colsample_bytree=colsample_bytree,\n",
        "            gamma=gamma,\n",
        "            reg_alpha=reg_alpha,\n",
        "            reg_lambda=reg_lambda,\n",
        "            eval_metric='error'  # Error metric for accuracy\n",
        "        )\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "\n",
        "    def fit(self):\n",
        "        self.model.fit(\n",
        "            self.train_x, self.train_y,\n",
        "            eval_set=[(self.train_x, self.train_y)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.model.predict(X).astype(float)\n",
        "\n",
        "    def calculate_accuracy(self, X, y, pred=None):\n",
        "        predictions = self.model.predict(X).astype(float) if pred is None else pred\n",
        "        accuracy = accuracy_score(y, predictions)\n",
        "        return accuracy\n",
        "\n",
        "    def plot_training_curve(self):\n",
        "        evals_result = self.model.evals_result()\n",
        "        error_values = evals_result['validation_0']['error']\n",
        "        accuracy_values = [1 - e for e in error_values]  # Convert error to accuracy\n",
        "        epochs = len(accuracy_values)\n",
        "        x_axis = range(0, epochs)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(x_axis, accuracy_values, marker='o', linestyle='-')\n",
        "        plt.title('XGBoost Training Accuracy Curve')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgeswX8TmWfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TfbVJt-wjpUL",
        "outputId": "1dbe9288-408e-4766-dcb1-d075513b813b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.9465360045433044\n",
            "Epoch 5, Loss: 0.8004679679870605\n",
            "Epoch 10, Loss: 0.7337527275085449\n",
            "Epoch 15, Loss: 0.6813313364982605\n",
            "Epoch 20, Loss: 0.6381024122238159\n",
            "Epoch 25, Loss: 0.5998936891555786\n",
            "Epoch 30, Loss: 0.5630395412445068\n",
            "Epoch 35, Loss: 0.5268213748931885\n",
            "Epoch 40, Loss: 0.49174249172210693\n",
            "Epoch 45, Loss: 0.45580756664276123\n",
            "<class 'numpy.ndarray'>\n",
            "[0. 1.]\n",
            "0.9475942487917364\n"
          ]
        }
      ],
      "source": [
        "ann_model = ANN(train_x, train_y)\n",
        "train_model_pt(ann_model)\n",
        "acc = predict_model_pt(ann_model, ann_model.train_x)\n",
        "print(type(acc))\n",
        "print(np.unique(acc))\n",
        "print(calculate_accuracy_pt(ann_model, ann_model.train_x, ann_model.train_y, acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3lXSu6E8sWNX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b18766-645a-429b-b40d-75a084f21d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[0. 1.]\n",
            "0.9285846094466427\n"
          ]
        }
      ],
      "source": [
        "svm = SVMClassifier(train_x, train_y)\n",
        "svm.fit()\n",
        "acc = svm.predict(svm.train_x)\n",
        "print(type(acc))\n",
        "print(np.unique(acc))\n",
        "print(svm.calculate_accuracy(svm.train_x, svm.train_y, acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm.plot_training_curve()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "5xTQJv6Tr9Os",
        "outputId": "2ff39d93-93fc-436d-afb8-9caae04c5862"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jXeSfWlQPIWy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fkw5jbXSIpfK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import inspect\n",
        "\n",
        "class Adaboost():\n",
        "    def __init__(self, classes_dict, train_x, train_y):\n",
        "        self.classes_dict = classes_dict\n",
        "        self.model_order = list(classes_dict.keys())\n",
        "        self.train_x = train_x\n",
        "        self.train_y = train_y\n",
        "        self.trained_model = {}\n",
        "        self.training_data_history = {'base': {'X': self.train_x, 'y': self.train_y}}\n",
        "        self.current_weight = None\n",
        "        self.weight_history = {}\n",
        "        self.restart = False\n",
        "        self.predictions = None\n",
        "\n",
        "    def weight_init(self): self.current_weight = pd.Series(np.ones(len(self.train_y)) / len(self.train_y))\n",
        "\n",
        "    def weight_calculate(self, predictions, labels):\n",
        "        incorrect = predictions != labels.to_numpy()\n",
        "        error_rate = self.current_weight[incorrect].sum()\n",
        "\n",
        "        print('Error rate is:', error_rate)\n",
        "\n",
        "        if error_rate > 0.5:\n",
        "            self.weight_init()\n",
        "            self.restart = True\n",
        "            return\n",
        "\n",
        "        alpha = 0.5 * np.log((1 - error_rate) / error_rate)\n",
        "\n",
        "        # Update weights\n",
        "        self.current_weight[incorrect] *= np.exp(alpha)\n",
        "        self.current_weight[~incorrect] *= np.exp(-alpha)\n",
        "\n",
        "        # Normalize weights\n",
        "        self.current_weight /= self.current_weight.sum()\n",
        "\n",
        "    def training(self):\n",
        "        self.weight_init()\n",
        "        for model in self.model_order:\n",
        "            while True:\n",
        "                self.restart = False\n",
        "                self.train_x['weight'] = self.current_weight\n",
        "                self.train_y = self.train_y.to_frame()\n",
        "                self.train_y['weight'] = self.current_weight\n",
        "                sampled_train_x = self.train_x.sample(n=len(self.train_x), replace=True, weights='weight', random_state=42)\n",
        "                del self.train_x['weight']\n",
        "                del sampled_train_x['weight']\n",
        "                sampled_train_x.sort_index(inplace=True)\n",
        "                sampled_train_x.reset_index(drop=True, inplace=True)\n",
        "                sampled_train_y = self.train_y.sample(n=len(self.train_y), replace=True, weights='weight', random_state=42)\n",
        "                del self.train_y['weight']\n",
        "                self.train_y = self.train_y.iloc[:, 0]\n",
        "                del sampled_train_y['weight']\n",
        "                sampled_train_y.sort_index(inplace=True)\n",
        "                sampled_train_y.reset_index(drop=True, inplace=True)\n",
        "                sampled_train_y = sampled_train_y.iloc[:, 0]\n",
        "\n",
        "                print(\"*\" * 37)\n",
        "                print(f'Training --------------------- {model}')\n",
        "                current_model = self.classes_dict[model](sampled_train_x, sampled_train_y)\n",
        "                methods = inspect.getmembers(current_model, predicate=inspect.ismethod)\n",
        "\n",
        "                if 'fit' in [z for z, _ in methods]:\n",
        "                    current_model.fit()\n",
        "                    print('Finish training.\\nStart predicting.')\n",
        "                    current_prediction = current_model.predict(current_model.train_x)\n",
        "                    train_accuracy = current_model.calculate_accuracy(current_model.train_x , current_model.train_y, current_prediction)\n",
        "                else:\n",
        "                    current_model.to(device)\n",
        "                    train_model_pt(current_model)\n",
        "                    print('Finish training.\\nStart predicting.')\n",
        "                    current_prediction = predict_model_pt(current_model, current_model.train_x)\n",
        "                    train_accuracy = calculate_accuracy_pt(current_model, current_model.train_x, current_model.train_y, current_prediction)\n",
        "\n",
        "                print(f'{model} training accuracy:', train_accuracy)\n",
        "                self.weight_calculate(current_prediction, sampled_train_y)\n",
        "\n",
        "                if not self.restart:\n",
        "                    self.train_x, self.train_y = sampled_train_x, sampled_train_y\n",
        "                    self.training_data_history[model] = {'X': sampled_train_x, 'y': sampled_train_y}\n",
        "                    self.trained_model[model] = current_model\n",
        "                    break\n",
        "    def reorder(self, order_list): self.model_order = order_list\n",
        "    def predict(self, X):\n",
        "        # Ensure X is a pandas DataFrame\n",
        "        assert isinstance(X, pd.DataFrame), \"Input X should be a pandas DataFrame\"\n",
        "\n",
        "        # Collect predictions from each trained model\n",
        "        model_predictions = {}\n",
        "        for model_name, model in self.trained_model.items():\n",
        "            methods = inspect.getmembers(model, predicate=inspect.ismethod)\n",
        "            if 'predict' in [z for z, _ in methods]: preds = model.predict(X)\n",
        "            else: preds = predict_model_pt(model, torch.tensor(X.to_numpy(), dtype=torch.float32).to(device))\n",
        "            assert isinstance(preds, np.ndarray), f\"Predictions from {model_name} should be a numpy array\"\n",
        "            if not np.array_equal(np.unique(preds), [0, 1]):\n",
        "                print('Alert --- ', np.unique(preds))\n",
        "                preds = np.where(preds > 0.5, 1, 0)\n",
        "            model_predictions[model_name] = preds\n",
        "        # Voting mechanism\n",
        "        predictions = np.zeros(len(X))\n",
        "        for i in range(len(X)):\n",
        "            votes = {}\n",
        "            for model_name, preds in model_predictions.items():\n",
        "                pred = preds[i]\n",
        "                if pred in votes: votes[pred] += 1\n",
        "                else: votes[pred] = 1\n",
        "            predictions[i] = max(votes, key=votes.get)\n",
        "        assert isinstance(predictions, np.ndarray), \"Final predictions should be a numpy array\"\n",
        "\n",
        "        self.predictions = predictions\n",
        "        return predictions\n",
        "    def calculate_accuracy(self, y):\n",
        "        assert isinstance(self.predictions, np.ndarray), \"Predictions should be a numpy array\"\n",
        "        assert isinstance(y, pd.Series), \"y should be a pandas Series\"\n",
        "        assert len(self.predictions) == len(y), \"Predictions and y should have the same length\"\n",
        "        accuracy = np.mean(self.predictions == y.to_numpy())\n",
        "        return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ2vWXWUydAs",
        "outputId": "d5254fd8-11cd-44ac-c351-85d3ec788bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*************************************\n",
            "Training --------------------- SVM\n",
            "Finish training.\n",
            "Start predicting.\n",
            "SVM training accuracy: 0.9295861547281812\n",
            "Error rate is: 0.07041384527181883\n",
            "*************************************\n",
            "Training --------------------- XGB\n",
            "Finish training.\n",
            "Start predicting.\n",
            "XGB training accuracy: 0.8358001022674929\n",
            "Error rate is: 0.1523307011823026\n",
            "*************************************\n",
            "Training --------------------- LR\n",
            "Epoch 0, Loss: 0.7153465747833252\n",
            "Epoch 5, Loss: 0.7112652063369751\n",
            "Epoch 10, Loss: 0.7074341773986816\n",
            "Epoch 15, Loss: 0.7038583159446716\n",
            "Epoch 20, Loss: 0.6988804936408997\n",
            "Epoch 25, Loss: 0.6956745386123657\n",
            "Epoch 30, Loss: 0.6928190588951111\n",
            "Epoch 35, Loss: 0.6901583075523376\n",
            "Epoch 40, Loss: 0.6876857876777649\n",
            "Epoch 45, Loss: 0.6853792667388916\n",
            "Epoch 50, Loss: 0.6832209229469299\n",
            "Epoch 55, Loss: 0.6811951994895935\n",
            "Epoch 60, Loss: 0.6792877316474915\n",
            "Epoch 65, Loss: 0.6774865388870239\n",
            "Epoch 70, Loss: 0.6757814884185791\n",
            "Epoch 75, Loss: 0.6741644740104675\n",
            "Epoch 80, Loss: 0.6726284027099609\n",
            "Epoch 85, Loss: 0.6711674928665161\n",
            "Epoch 90, Loss: 0.6697757840156555\n",
            "Epoch 95, Loss: 0.6684485673904419\n",
            "Epoch 100, Loss: 0.6671810150146484\n",
            "Epoch 105, Loss: 0.6659688949584961\n",
            "Epoch 110, Loss: 0.6648086309432983\n",
            "Epoch 115, Loss: 0.6636971235275269\n",
            "Epoch 120, Loss: 0.6626315116882324\n",
            "Epoch 125, Loss: 0.661609411239624\n",
            "Epoch 130, Loss: 0.6606274247169495\n",
            "Epoch 135, Loss: 0.6596822142601013\n",
            "Epoch 140, Loss: 0.6587702035903931\n",
            "Epoch 145, Loss: 0.6578888893127441\n",
            "Epoch 150, Loss: 0.6570361256599426\n",
            "Epoch 155, Loss: 0.6562100052833557\n",
            "Epoch 160, Loss: 0.6554091572761536\n",
            "Epoch 165, Loss: 0.6546322107315063\n",
            "Epoch 170, Loss: 0.6538777947425842\n",
            "Epoch 175, Loss: 0.6531449556350708\n",
            "Epoch 180, Loss: 0.6524324417114258\n",
            "Epoch 185, Loss: 0.6517394185066223\n",
            "Epoch 190, Loss: 0.6510648727416992\n",
            "Epoch 195, Loss: 0.6504080295562744\n",
            "Epoch 200, Loss: 0.6497679352760315\n",
            "Epoch 205, Loss: 0.6491441130638123\n",
            "Epoch 210, Loss: 0.6485356092453003\n",
            "Epoch 215, Loss: 0.6479418873786926\n",
            "Epoch 220, Loss: 0.647362470626831\n",
            "Epoch 225, Loss: 0.6467965245246887\n",
            "Epoch 230, Loss: 0.6462438106536865\n",
            "Epoch 235, Loss: 0.6457036137580872\n",
            "Epoch 240, Loss: 0.645175576210022\n",
            "Epoch 245, Loss: 0.644659161567688\n",
            "Epoch 250, Loss: 0.6441539525985718\n",
            "Epoch 255, Loss: 0.6436596512794495\n",
            "Epoch 260, Loss: 0.6431758999824524\n",
            "Epoch 265, Loss: 0.6427022218704224\n",
            "Epoch 270, Loss: 0.6422383189201355\n",
            "Epoch 275, Loss: 0.6417838931083679\n",
            "Epoch 280, Loss: 0.6413386464118958\n",
            "Epoch 285, Loss: 0.6409022808074951\n",
            "Epoch 290, Loss: 0.6404744982719421\n",
            "Epoch 295, Loss: 0.6400550603866577\n",
            "Epoch 300, Loss: 0.639643669128418\n",
            "Epoch 305, Loss: 0.6392400860786438\n",
            "Epoch 310, Loss: 0.6388441324234009\n",
            "Epoch 315, Loss: 0.6384555697441101\n",
            "Epoch 320, Loss: 0.6380741000175476\n",
            "Epoch 325, Loss: 0.6376996040344238\n",
            "Epoch 330, Loss: 0.6373319029808044\n",
            "Epoch 335, Loss: 0.6369706392288208\n",
            "Epoch 340, Loss: 0.6366158723831177\n",
            "Epoch 345, Loss: 0.6362672448158264\n",
            "Epoch 350, Loss: 0.6359245181083679\n",
            "Epoch 355, Loss: 0.635587751865387\n",
            "Epoch 360, Loss: 0.6352565884590149\n",
            "Epoch 365, Loss: 0.6349310278892517\n",
            "Epoch 370, Loss: 0.6346108317375183\n",
            "Epoch 375, Loss: 0.6342958807945251\n",
            "Epoch 380, Loss: 0.6339860558509827\n",
            "Epoch 385, Loss: 0.6336809992790222\n",
            "Epoch 390, Loss: 0.6333809494972229\n",
            "Epoch 395, Loss: 0.6330855488777161\n",
            "Epoch 400, Loss: 0.6327946186065674\n",
            "Epoch 405, Loss: 0.6325082182884216\n",
            "Epoch 410, Loss: 0.6322261095046997\n",
            "Epoch 415, Loss: 0.6319483518600464\n",
            "Epoch 420, Loss: 0.6316745281219482\n",
            "Epoch 425, Loss: 0.6314048767089844\n",
            "Epoch 430, Loss: 0.6311390995979309\n",
            "Epoch 435, Loss: 0.6308770179748535\n",
            "Epoch 440, Loss: 0.6306187510490417\n",
            "Epoch 445, Loss: 0.6303641200065613\n",
            "Epoch 450, Loss: 0.6301129460334778\n",
            "Epoch 455, Loss: 0.629865288734436\n",
            "Epoch 460, Loss: 0.6296209692955017\n",
            "Epoch 465, Loss: 0.6293799877166748\n",
            "Epoch 470, Loss: 0.6291422247886658\n",
            "Epoch 475, Loss: 0.6289075613021851\n",
            "Epoch 480, Loss: 0.6286759376525879\n",
            "Epoch 485, Loss: 0.6284472942352295\n",
            "Epoch 490, Loss: 0.6282216310501099\n",
            "Epoch 495, Loss: 0.6279987692832947\n",
            "Epoch 500, Loss: 0.6277787089347839\n",
            "Epoch 505, Loss: 0.6275614500045776\n",
            "Epoch 510, Loss: 0.6273468136787415\n",
            "Epoch 515, Loss: 0.6271348595619202\n",
            "Epoch 520, Loss: 0.6269253492355347\n",
            "Epoch 525, Loss: 0.6267184615135193\n",
            "Epoch 530, Loss: 0.6265139579772949\n",
            "Epoch 535, Loss: 0.6263118982315063\n",
            "Epoch 540, Loss: 0.6261121034622192\n",
            "Epoch 545, Loss: 0.6259147524833679\n",
            "Epoch 550, Loss: 0.6257196664810181\n",
            "Epoch 555, Loss: 0.6255266070365906\n",
            "Epoch 560, Loss: 0.6253358125686646\n",
            "Epoch 565, Loss: 0.6251472234725952\n",
            "Epoch 570, Loss: 0.6249605417251587\n",
            "Epoch 575, Loss: 0.6247761845588684\n",
            "Epoch 580, Loss: 0.6245935559272766\n",
            "Epoch 585, Loss: 0.6244130730628967\n",
            "Epoch 590, Loss: 0.6242344975471497\n",
            "Epoch 595, Loss: 0.6240577697753906\n",
            "Epoch 600, Loss: 0.6238828897476196\n",
            "Epoch 605, Loss: 0.6237099766731262\n",
            "Epoch 610, Loss: 0.6235387325286865\n",
            "Epoch 615, Loss: 0.6233692169189453\n",
            "Epoch 620, Loss: 0.6232015490531921\n",
            "Epoch 625, Loss: 0.6230356097221375\n",
            "Epoch 630, Loss: 0.6228712797164917\n",
            "Epoch 635, Loss: 0.6227085590362549\n",
            "Epoch 640, Loss: 0.6225475668907166\n",
            "Epoch 645, Loss: 0.6223880648612976\n",
            "Epoch 650, Loss: 0.6222301721572876\n",
            "Epoch 655, Loss: 0.6220738291740417\n",
            "Epoch 660, Loss: 0.6219190359115601\n",
            "Epoch 665, Loss: 0.6217657327651978\n",
            "Epoch 670, Loss: 0.6216138601303101\n",
            "Epoch 675, Loss: 0.6214633584022522\n",
            "Epoch 680, Loss: 0.6213144659996033\n",
            "Epoch 685, Loss: 0.6211668848991394\n",
            "Epoch 690, Loss: 0.6210206151008606\n",
            "Epoch 695, Loss: 0.6208757758140564\n",
            "Epoch 700, Loss: 0.620732307434082\n",
            "Epoch 705, Loss: 0.620590090751648\n",
            "Epoch 710, Loss: 0.6204491853713989\n",
            "Epoch 715, Loss: 0.620309591293335\n",
            "Epoch 720, Loss: 0.6201712489128113\n",
            "Epoch 725, Loss: 0.6200341582298279\n",
            "Epoch 730, Loss: 0.6198983192443848\n",
            "Epoch 735, Loss: 0.6197636127471924\n",
            "Epoch 740, Loss: 0.6196302175521851\n",
            "Epoch 745, Loss: 0.6194978952407837\n",
            "Epoch 750, Loss: 0.6193667054176331\n",
            "Epoch 755, Loss: 0.6192367672920227\n",
            "Epoch 760, Loss: 0.6191078424453735\n",
            "Epoch 765, Loss: 0.6189800500869751\n",
            "Epoch 770, Loss: 0.6188533902168274\n",
            "Epoch 775, Loss: 0.6187278628349304\n",
            "Epoch 780, Loss: 0.6186033487319946\n",
            "Epoch 785, Loss: 0.6184799075126648\n",
            "Epoch 790, Loss: 0.6183575391769409\n",
            "Epoch 795, Loss: 0.6182360649108887\n",
            "Epoch 800, Loss: 0.6181157231330872\n",
            "Epoch 805, Loss: 0.617996335029602\n",
            "Epoch 810, Loss: 0.6178779006004333\n",
            "Epoch 815, Loss: 0.6177605390548706\n",
            "Epoch 820, Loss: 0.6176441311836243\n",
            "Epoch 825, Loss: 0.6175286173820496\n",
            "Epoch 830, Loss: 0.6174139976501465\n",
            "Epoch 835, Loss: 0.6173003315925598\n",
            "Epoch 840, Loss: 0.6171876788139343\n",
            "Epoch 845, Loss: 0.6170759201049805\n",
            "Epoch 850, Loss: 0.6169649362564087\n",
            "Epoch 855, Loss: 0.6168548464775085\n",
            "Epoch 860, Loss: 0.6167457699775696\n",
            "Epoch 865, Loss: 0.6166374683380127\n",
            "Epoch 870, Loss: 0.6165299415588379\n",
            "Epoch 875, Loss: 0.6164233088493347\n",
            "Epoch 880, Loss: 0.6163174510002136\n",
            "Epoch 885, Loss: 0.6162125468254089\n",
            "Epoch 890, Loss: 0.6161083579063416\n",
            "Epoch 895, Loss: 0.616005003452301\n",
            "Epoch 900, Loss: 0.6159024834632874\n",
            "Epoch 905, Loss: 0.6158006191253662\n",
            "Epoch 910, Loss: 0.6156996488571167\n",
            "Epoch 915, Loss: 0.6155993938446045\n",
            "Epoch 920, Loss: 0.6154998540878296\n",
            "Epoch 925, Loss: 0.6154010891914368\n",
            "Epoch 930, Loss: 0.615303099155426\n",
            "Epoch 935, Loss: 0.6152058243751526\n",
            "Epoch 940, Loss: 0.6151093244552612\n",
            "Epoch 945, Loss: 0.6150134801864624\n",
            "Epoch 950, Loss: 0.6149182915687561\n",
            "Epoch 955, Loss: 0.6148238778114319\n",
            "Epoch 960, Loss: 0.6147300601005554\n",
            "Epoch 965, Loss: 0.614637017250061\n",
            "Epoch 970, Loss: 0.6145446300506592\n",
            "Epoch 975, Loss: 0.6144528985023499\n",
            "Epoch 980, Loss: 0.6143618226051331\n",
            "Epoch 985, Loss: 0.6142714619636536\n",
            "Epoch 990, Loss: 0.614181637763977\n",
            "Epoch 995, Loss: 0.6140925884246826\n",
            "Epoch 1000, Loss: 0.6140040755271912\n",
            "Epoch 1005, Loss: 0.6139162182807922\n",
            "Epoch 1010, Loss: 0.6138289570808411\n",
            "Epoch 1015, Loss: 0.6137422919273376\n",
            "Epoch 1020, Loss: 0.6136562824249268\n",
            "Epoch 1025, Loss: 0.6135708689689636\n",
            "Epoch 1030, Loss: 0.6134859919548035\n",
            "Epoch 1035, Loss: 0.6134017705917358\n",
            "Epoch 1040, Loss: 0.613318145275116\n",
            "Epoch 1045, Loss: 0.6132349967956543\n",
            "Epoch 1050, Loss: 0.6131525039672852\n",
            "Epoch 1055, Loss: 0.6130706071853638\n",
            "Epoch 1060, Loss: 0.6129891872406006\n",
            "Epoch 1065, Loss: 0.6129083037376404\n",
            "Epoch 1070, Loss: 0.6128280758857727\n",
            "Epoch 1075, Loss: 0.6127482652664185\n",
            "Epoch 1080, Loss: 0.612669050693512\n",
            "Epoch 1085, Loss: 0.6125903725624084\n",
            "Epoch 1090, Loss: 0.6125122904777527\n",
            "Epoch 1095, Loss: 0.6124346256256104\n",
            "Epoch 1100, Loss: 0.612357497215271\n",
            "Epoch 1105, Loss: 0.6122809052467346\n",
            "Epoch 1110, Loss: 0.6122048497200012\n",
            "Epoch 1115, Loss: 0.6121292114257812\n",
            "Epoch 1120, Loss: 0.6120541095733643\n",
            "Epoch 1125, Loss: 0.6119794845581055\n",
            "Epoch 1130, Loss: 0.6119053959846497\n",
            "Epoch 1135, Loss: 0.6118317246437073\n",
            "Epoch 1140, Loss: 0.6117585897445679\n",
            "Epoch 1145, Loss: 0.6116858720779419\n",
            "Epoch 1150, Loss: 0.6116135716438293\n",
            "Epoch 1155, Loss: 0.6115418076515198\n",
            "Epoch 1160, Loss: 0.6114705801010132\n",
            "Epoch 1165, Loss: 0.6113997101783752\n",
            "Epoch 1170, Loss: 0.6113293170928955\n",
            "Epoch 1175, Loss: 0.611259400844574\n",
            "Epoch 1180, Loss: 0.6111899018287659\n",
            "Epoch 1185, Loss: 0.6111207604408264\n",
            "Epoch 1190, Loss: 0.6110522150993347\n",
            "Epoch 1195, Loss: 0.6109840273857117\n",
            "Finish training.\n",
            "Start predicting.\n",
            "LR training accuracy: 0.6399550361105673\n",
            "Error rate is: 0.347934364946604\n",
            "*************************************\n",
            "Training --------------------- ANN\n"
          ]
        }
      ],
      "source": [
        "adModel = Adaboost(classes_dict={\"LR\": LogisticRegressionModel, \"SVM\": SVMClassifier, \"ANN\": ANN, \"NB\": NaiveBayesClassifier, \"RF\": RandomForestModel, \"XGB\": XGBoostClassifier}, train_x=train_x, train_y=train_y)\n",
        "adModel.reorder(['SVM', 'XGB', 'ANN', 'RF'])\n",
        "adModel.training()\n",
        "print(\"*\" * 37)\n",
        "print(\"Adaboost model prediction: \", adModel.predict(train_x))\n",
        "print(\"Adaboost model training accuracy: \", adModel.calculate_accuracy(train_y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4olt-V4uO4JA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}