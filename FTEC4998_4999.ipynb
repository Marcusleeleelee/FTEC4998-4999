{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdZ3xHMz5/By/aSActN/e5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcusleeleelee/FTEC4998-4999/blob/main/FTEC4998_4999.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Import the packages - ok\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from time import sleep\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9Irism69Bo6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5579d87-efaf-403c-adeb-c7c94875a736"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Utils - ok\n",
        "def uni_list(input): return list(set(input))\n",
        "def perform_pca(df, n_components):\n",
        "\n",
        "    # Performing PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    principal_components = pca.fit_transform(df)\n",
        "\n",
        "    # Creating a DataFrame with the top 15 components\n",
        "    pca_df = pd.DataFrame(data=principal_components, index=df.index)\n",
        "\n",
        "    # Retaining the original column names for the new DataFrame\n",
        "    retained_columns = df.columns[:n_components]\n",
        "    pca_df.columns = retained_columns\n",
        "\n",
        "    return pca_df"
      ],
      "metadata": {
        "id": "ndu77M8kBr-h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Dataset\n",
        "class Dataset:\n",
        "    def __init__(self, file_path): # ok\n",
        "        self.dataset = pd.read_feather(file_path)\n",
        "        self.train_dict, self.test_dict = {}, {}\n",
        "        self.scalers = None\n",
        "        self.pca = None\n",
        "        self.label = 'loan_condition_cat'\n",
        "\n",
        "    def show(self, rows=10): # ok\n",
        "        return self.dataset.head(rows)\n",
        "\n",
        "    def get(self, type, key): # ok\n",
        "        if type == 'test': return self.test_dict[key]['x'], self.test_dict[key]['y']\n",
        "        elif type == 'train': return self.train_dict[key]['x'], self.train_dict[key]['y']\n",
        "        else: raise Exception('The type must be either \"test\" or \"train\"!')\n",
        "\n",
        "    def update(self, type, key, x, y): # ok\n",
        "        if type == 'test': self.test_dict.update({key: {'x': x, 'y': y}})\n",
        "        elif type == 'train': self.train_dict.update({key: {'x': x, 'y': y}})\n",
        "        else: raise Exception('The type must be either \"test\" or \"train\"!')\n",
        "\n",
        "    def basic_processing(self): # ok\n",
        "        temp_func_1 = lambda x: '<=2009' if str(x) in ['2007', '2008', '2009'] else (\"[2010, 2012]\" if str(x) in ['2010', '2011', '2012'] else '>=2013')\n",
        "        columns_to_delete = [\n",
        "            'id', 'issue_d', 'home_ownership_cat', 'income_category', 'income_cat', 'term_cat', 'application_type_cat',\n",
        "            'purpose_cat', 'interest_payment_cat', 'loan_condition'\n",
        "        ]\n",
        "        self.dataset.drop(columns=columns_to_delete, inplace=True)\n",
        "        self.dataset['grade'] = self.dataset['grade'].apply(temp_func_1)\n",
        "        self.dataset['final_d'] = self.dataset['final_d'].apply(lambda x: str(x)[-4:]).apply(temp_func_1)\n",
        "        self.dataset = pd.get_dummies(self.dataset, columns=['year', 'final_d', 'home_ownership', 'term', 'application_type',\n",
        "                                                             'purpose', 'interest_payments', 'grade', 'region'], dtype=int)\n",
        "\n",
        "    def train_test_split(self, percentage=0.8): # ok\n",
        "        self.dataset = self.dataset.sample(frac=1).reset_index(drop=True)\n",
        "        train_size = int(len(self.dataset) * percentage)\n",
        "        temp1 = self.dataset.iloc[:train_size].copy()\n",
        "        temp2 = self.dataset.iloc[train_size:].copy()\n",
        "\n",
        "        y_train = temp1[[self.label]]\n",
        "        x_train = temp1.drop(columns=[self.label])\n",
        "        self.update('train', 'w0', x_train, y_train)\n",
        "\n",
        "        y_test = temp2[[self.label]]\n",
        "        x_test = temp2.drop(columns=[self.label])\n",
        "        self.update('test', 'w0', x_test, y_test)\n",
        "\n",
        "    def preprocessing_train(self): # ok\n",
        "        temp_train_x, temp_train_y = self.get('train', 'w0')\n",
        "        scaler = StandardScaler()\n",
        "        temp_train_x = pd.DataFrame(scaler.fit_transform(temp_train_x), columns=temp_train_x.columns)\n",
        "        self.scalers = scaler\n",
        "        temp_train_x = perform_pca(temp_train_x, n_components=30)\n",
        "        self.update('train', 'w0', temp_train_x, temp_train_y)\n",
        "\n",
        "    def preprocessing_test(self): # Not ok\n",
        "        temp_test_x, temp_test_y = self.get('test', 'w0')\n",
        "        temp_train_x, _ = self.get('train', 'w0')\n",
        "        print(temp_train_x)\n",
        "\n",
        "        # Apply stored scalers\n",
        "        temp_test_x = pd.DataFrame(self.scalers.transform(temp_test_x), columns=temp_train_x.columns)\n",
        "\n",
        "        assert set(temp_test_x.columns) == set(temp_train_x.columns)\n",
        "        self.update('test', 'w0', temp_test_x, temp_test_y)"
      ],
      "metadata": {
        "id": "lmLx5bbgBtcq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating # ok\n",
        "data = Dataset('/content/drive/My Drive/Colab Notebooks/FTEC4998_9/loan_final313_processed.feather')\n",
        "data.basic_processing()\n",
        "data.train_test_split()\n",
        "data.preprocessing_train()"
      ],
      "metadata": {
        "id": "13kLSUwPBvO9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data conversion # ok\n",
        "train_x, train_y = data.get('train', 'w0')\n",
        "X_train, y_train = train_x.to_numpy(), train_y.values.ravel()\n",
        "counts = np.mean(y_train == 1) * 100\n",
        "print(counts)\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "EMPCAhUjB4tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c465b29-8b80-40d8-a71b-865e6784ffe4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.566385830176799\n",
            "(709903, 30) (709903,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Model training\n",
        "# Ensure y_train is binary\n",
        "assert set(y_train).issubset({0, 1}), \"Target values must be 0 or 1 for binary classification.\"\n",
        "\n",
        "# Convert to tensors and move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "# Convert y_train to -1 and 1 for SVM\n",
        "y_train_svm_tensor = y_train_tensor * 2 - 1\n",
        "\n",
        "# Logistic Regression as a neural network\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, X_train_tensor, y_train_tensor, lr=0.001):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.input_dim = X_train_tensor.shape[1]\n",
        "        self.linear = nn.Linear(self.input_dim, 1)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=lr)\n",
        "        self.epochs = 100\n",
        "        self.y_train_tensor = y_train_tensor\n",
        "        self.X_train_tensor = X_train_tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "# MLP model\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, X_train_tensor, y_train_tensor, lr=0.001):\n",
        "        super(ANN, self).__init__()\n",
        "        self.input_dim = X_train_tensor.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 100\n",
        "        self.y_train_tensor = y_train_tensor\n",
        "        self.X_train_tensor = X_train_tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# SVM Model\n",
        "class SVM(nn.Module):\n",
        "    def __init__(self, X_train_tensor, y_train_tensor, lr=0.001):\n",
        "        super(SVM, self).__init__()\n",
        "        self.input_dim = X_train_tensor.shape[1]\n",
        "        self.linear = nn.Linear(self.input_dim, 1)\n",
        "        self.criterion = nn.HingeEmbeddingLoss()\n",
        "        self.optimizer = optim.AdamW(self.parameters(), lr=lr)\n",
        "        self.epochs = 100\n",
        "        self.y_train_tensor = y_train_tensor * 2 - 1\n",
        "        self.X_train_tensor = X_train_tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "class NaiveBayes:\n",
        "    def __init__(self, device, X_train_tensor, y_train_tensor):\n",
        "        self.classes = None\n",
        "        self.class_priors = None\n",
        "        self.feature_probs = None\n",
        "        self.device = device\n",
        "        self.X_train_tensor = X_train_tensor\n",
        "        self.y_train_tensor = y_train_tensor\n",
        "\n",
        "    def fit(self):\n",
        "        y_train_tensor = self.y_train_tensor.squeeze().long()\n",
        "        self.classes, class_counts = torch.unique(y_train_tensor, return_counts=True)\n",
        "        self.class_priors = class_counts.float() / y_train_tensor.size(0)\n",
        "        self.feature_probs = []\n",
        "        for c in self.classes:\n",
        "            X_c = self.X_train_tensor[y_train_tensor == c]\n",
        "            class_feature_prob = (X_c.sum(dim=0) + 1) / (X_c.sum() + X_c.size(1))\n",
        "            self.feature_probs.append(class_feature_prob)\n",
        "        self.feature_probs = torch.stack(self.feature_probs).to(self.device)\n",
        "\n",
        "    def predict(self, X_tensor):\n",
        "        log_probs = []\n",
        "        for i, c in enumerate(self.classes):\n",
        "            log_prior = torch.log(self.class_priors[i])\n",
        "            log_likelihood = (\n",
        "                X_tensor * torch.log(self.feature_probs[i]) +\n",
        "                (1 - X_tensor) * torch.log(1 - self.feature_probs[i])\n",
        "            )\n",
        "            log_probs.append(log_prior + log_likelihood.sum(dim=1))\n",
        "        log_probs = torch.stack(log_probs).T\n",
        "        return self.classes[log_probs.argmax(dim=1)].cpu().numpy().astype(float)\n",
        "\n",
        "# Train and predict functions\n",
        "def train_model(model):\n",
        "    model.train()\n",
        "    for epoch in range(model.epochs):\n",
        "        model.optimizer.zero_grad()\n",
        "        outputs = model(model.X_train_tensor)\n",
        "        loss = model.criterion(outputs, model.y_train_tensor)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "\n",
        "def predict_model(model, X_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_tensor).squeeze()\n",
        "        if isinstance(model, SVM): return (outputs > 0).float()  # For SVM\n",
        "        else: return (outputs > 0.5).float()  # For Logistic Regression and ANN\n",
        "\n",
        "def calculate_accuracy(model, X_tensor, y_tensor):\n",
        "    # Ensure tensors are on the same device\n",
        "    if isinstance(model, NaiveBayes):\n",
        "        X_tensor = X_tensor.to(model.device)\n",
        "        y_tensor = y_tensor.to(model.device).squeeze()\n",
        "        predictions = model.predict(X_tensor)\n",
        "    else:\n",
        "        X_tensor = X_tensor.to(next(model.parameters()).device)\n",
        "        y_tensor = y_tensor.to(next(model.parameters()).device)\n",
        "        predictions = predict_model(model, X_tensor)\n",
        "\n",
        "\n",
        "    # Ensure predictions and labels are the same shape\n",
        "    predictions = predictions.squeeze()\n",
        "    y_tensor = y_tensor.squeeze()\n",
        "\n",
        "    correct = (predictions == y_tensor).sum().item()\n",
        "    accuracy = correct / y_tensor.size(0)\n",
        "    return accuracy\n",
        "\n",
        "# Initialize PyTorch models and move to GPU\n",
        "log_reg_model = LogisticRegressionModel(X_train_tensor, y_train_tensor).to(device)\n",
        "ann_model = ANN(X_train_tensor, y_train_tensor).to(device)\n",
        "svm_model = SVM(X_train_tensor, y_train_tensor).to(device)\n",
        "nb_model = NaiveBayes(device, X_train_tensor, y_train_tensor)\n",
        "print(log_reg_model.criterion)\n",
        "print(ann_model.criterion)\n",
        "print(svm_model.criterion)"
      ],
      "metadata": {
        "id": "KQxTOBOkHlsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f662d28f-1673-49c2-e881-d888ebaa5702"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BCELoss()\n",
            "BCELoss()\n",
            "HingeEmbeddingLoss()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train PyTorch models\n",
        "train_model(log_reg_model)\n",
        "train_model(ann_model)\n",
        "train_model(svm_model)\n",
        "nb_model.fit()"
      ],
      "metadata": {
        "id": "NadN225Ro4F2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict with PyTorch models and move predictions to CPU\n",
        "log_reg_predictions = predict_model(log_reg_model, X_train_tensor).cpu().numpy()\n",
        "ann_predictions = predict_model(ann_model, X_train_tensor).cpu().numpy()\n",
        "svm_predictions = predict_model(svm_model, X_train_tensor).cpu().numpy()\n",
        "nb_predictions = nb_model.predict(nb_model.X_train_tensor)"
      ],
      "metadata": {
        "id": "7xY6uPCbr9ru"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Accuracy:"
      ],
      "metadata": {
        "id": "o0DtUP13pGQm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect predictions for stacking\n",
        "predictions = {\n",
        "    \"log_reg\": log_reg_predictions,\n",
        "    \"ANN\": ann_predictions,\n",
        "    'svm':svm_predictions,\n",
        "    'nb':nb_predictions,\n",
        "}"
      ],
      "metadata": {
        "id": "ypQUg_R6samW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('ANN:', calculate_accuracy(ann_model, ann_model.X_train_tensor, ann_model.y_train_tensor))"
      ],
      "metadata": {
        "id": "1Wqxerwgrswx",
        "outputId": "899ad8dd-bc05-4a60-9112-6a3aa5cc7e06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN: 0.49298988735080707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Log_reg:', calculate_accuracy(log_reg_model, log_reg_model.X_train_tensor, log_reg_model.y_train_tensor))"
      ],
      "metadata": {
        "id": "8Qi2SnaBr13j",
        "outputId": "0ad8efc2-dfdc-4a68-a7c7-d9dae7c66df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log_reg: 0.594665750109522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('SVM:', calculate_accuracy(svm_model, svm_model.X_train_tensor, svm_model.y_train_tensor))"
      ],
      "metadata": {
        "id": "tRA5tKbwtH1c",
        "outputId": "cd224aa2-6c9c-40a8-f5fd-1ec1ba4b6f81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM: 0.043458049902592326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('NB:', calculate_accuracy(nb_model, nb_model.X_train_tensor, nb_model.y_train_tensor))"
      ],
      "metadata": {
        "id": "WVVJvrBWtONM",
        "outputId": "bebbe99b-4b93-434d-f5f8-c1bd487702f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-95500323c67e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NB:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-a943f26dbe9e>\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[0;34m(model, X_tensor, y_tensor)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mX_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0my_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mX_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-a943f26dbe9e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X_tensor)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_priors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             log_likelihood = (\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack predictions as features for AdaBoost\n",
        "stacked_features = np.column_stack(list(predictions.values()))\n",
        "\n",
        "# Initialize and train AdaBoost\n",
        "ada_boost = AdaBoostClassifier(n_estimators=30, random_state=29)\n",
        "ada_boost.fit(stacked_features, y_train)\n",
        "\n",
        "# Evaluate train\n",
        "stacked_accuracy = ada_boost.score(stacked_features, y_train)\n",
        "print(\"Stacked model accuracy:\", stacked_accuracy)"
      ],
      "metadata": {
        "id": "9YtaBSy9ZyUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkw5jbXSIpfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PK_0JVF6IZu-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}