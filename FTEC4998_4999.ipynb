{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7uqfu1zV9ewF8iddsU2VH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcusleeleelee/FTEC4998-4999/blob/main/FTEC4998_4999.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Import the packages - ok\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "9Irism69Bo6L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Utils - ok\n",
        "def uni_list(input): return list(set(input))\n",
        "def perform_pca(df, n_components):\n",
        "\n",
        "    # Performing PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    principal_components = pca.fit_transform(df)\n",
        "\n",
        "    # Creating a DataFrame with the top 15 components\n",
        "    pca_df = pd.DataFrame(data=principal_components, index=df.index)\n",
        "\n",
        "    # Retaining the original column names for the new DataFrame\n",
        "    retained_columns = df.columns[:n_components]\n",
        "    pca_df.columns = retained_columns\n",
        "\n",
        "    return pca_df"
      ],
      "metadata": {
        "id": "ndu77M8kBr-h"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    def __init__(self, file_path):\n",
        "        self.dataset = pd.read_csv(file_path, low_memory=False)\n",
        "        self.train_dict, self.test_dict = {}, {}\n",
        "        self.scalers = None\n",
        "        self.pca = None\n",
        "        self.label = 'loan_condition_cat'\n",
        "\n",
        "    def show(self):\n",
        "        return self.dataset.head(10)\n",
        "\n",
        "    def get(self, type, key):\n",
        "        if type == 'test': return self.test_dict[key]['x'], self.test_dict[key]['y']\n",
        "        elif type == 'train': return self.train_dict[key]['x'], self.train_dict[key]['y']\n",
        "        else: raise Exception('The type must be either \"test\" or \"train\"!!')\n",
        "\n",
        "    def update(self, type, key, x, y):\n",
        "        if type == 'test': self.test_dict.update({key: {'x': x, 'y': y}})\n",
        "        elif type == 'train': self.train_dict.update({key: {'x': x, 'y': y}})\n",
        "        else: raise Exception('The type must be either \"test\" or \"train\"!!')\n",
        "\n",
        "    def basic_processing(self):\n",
        "        temp_func_1 = lambda x: '<=2009' if str(x) in ['2007', '2008', '2009'] else (\"[2010, 2012]\" if str(x) in ['2010', '2011', '2012'] else '>=2013')\n",
        "        columns_to_delete = [\n",
        "            'id', 'issue_d', 'home_ownership_cat', 'income_category', 'income_cat', 'term_cat', 'application_type_cat',\n",
        "            'purpose_cat', 'interest_payment_cat', 'loan_condition'\n",
        "        ]\n",
        "        self.dataset.drop(columns=columns_to_delete, inplace=True)\n",
        "        self.dataset['grade'] = self.dataset['grade'].apply(temp_func_1)\n",
        "        self.dataset['final_d'] = self.dataset['final_d'].apply(lambda x: str(x)[-4:]).apply(temp_func_1)\n",
        "        self.dataset = pd.get_dummies(self.dataset, columns=['year', 'final_d', 'home_ownership', 'term', 'application_type',\n",
        "                                                             'purpose', 'interest_payments', 'grade', 'region'], dtype=int)\n",
        "\n",
        "    def train_test_split(self, percentage=0.8):\n",
        "        self.dataset = self.dataset.sample(frac=1).reset_index(drop=True)\n",
        "        train_size = int(len(self.dataset) * percentage)\n",
        "        temp1 = self.dataset.iloc[:train_size].copy()\n",
        "        temp2 = self.dataset.iloc[train_size:].copy()\n",
        "\n",
        "        y_train = temp1[[self.label]]\n",
        "        x_train = temp1.drop(columns=[self.label])\n",
        "        self.update('train', 'w0', x_train, y_train)\n",
        "\n",
        "        y_test = temp2[[self.label]]\n",
        "        x_test = temp2.drop(columns=[self.label])\n",
        "        self.update('test', 'w0', x_test, y_test)\n",
        "\n",
        "    def preprocessing_train(self):\n",
        "        temp_train_x, temp_train_y = self.get('train', 'w0')\n",
        "        scaler = StandardScaler()\n",
        "        temp_train_x = pd.DataFrame(scaler.fit_transform(temp_train_x), columns=temp_train_x.columns)\n",
        "        self.scalers = scaler\n",
        "        temp_train_x = temp_train_x.fillna(temp_train_x.mean())\n",
        "        temp_train_x = perform_pca(temp_train_x, n_components=30)\n",
        "        self.update('train', 'w0', temp_train_x, temp_train_y)\n",
        "\n",
        "    def preprocessing_test(self): # Not ok\n",
        "        temp_test_x, temp_test_y = self.get('test', 'w0')\n",
        "        temp_train_x, _ = self.get('train', 'w0')\n",
        "        print(temp_train_x)\n",
        "\n",
        "        # Apply stored scalers\n",
        "        temp_test_x = pd.DataFrame(self.scalers.transform(temp_test_x), columns=temp_train_x.columns)\n",
        "\n",
        "        assert set(temp_test_x.columns) == set(temp_train_x.columns)\n",
        "        self.update('test', 'w0', temp_test_x, temp_test_y)\n",
        "\n",
        "    def resample_with_weights(self, model, weight): # Not ok\n",
        "        temp_train_x, temp_train_y = self.get('train', weight)\n",
        "        y_pred = model.predict(temp_train_x)\n",
        "\n",
        "        misclassified = (temp_train_y[self.label].values != y_pred)\n",
        "        weights = np.ones(len(temp_train_y))\n",
        "\n",
        "        if misclassified.any():\n",
        "            weights[misclassified] = 1.0 / misclassified.sum()\n",
        "        if (~misclassified).any():\n",
        "            weights[~misclassified] = 1.0 / (~misclassified).sum()\n",
        "\n",
        "        weights /= weights.sum()\n",
        "\n",
        "        sampled_indices = np.random.choice(temp_train_x.index, size=len(temp_train_x), replace=True, p=weights)\n",
        "        temp_x = temp_train_x.loc[sampled_indices]\n",
        "        temp_y = temp_train_y.loc[sampled_indices]\n",
        "        self.update('train', 'w' + str(int(weight[1:]) + 1), temp_x, temp_y)"
      ],
      "metadata": {
        "id": "lmLx5bbgBtcq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating\n",
        "data = Dataset('loan_final313.csv')\n",
        "data.basic_processing()\n",
        "data.train_test_split()\n",
        "data.preprocessing_train()"
      ],
      "metadata": {
        "id": "13kLSUwPBvO9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "train_x, train_y = data.get('train', 'w0')\n",
        "train_y = train_y.values.ravel()\n",
        "counts = np.mean(train_y == 1) * 100\n",
        "print(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMPCAhUjB4tJ",
        "outputId": "ab8c4771-1ff0-480d-a033-8c40c000e76d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13.83496222258365\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install RAPIDS in Colab\n",
        "!apt-get install -y python3-dev libnvvm3\n",
        "!pip install --upgrade pip\n",
        "!pip install cupy-cuda11x\n",
        "!pip install cuml-cu11 -f https://rapidsai.github.io/rapidsai-csp-utils/cu11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zu-HKdwCIlC",
        "outputId": "6729bfc9-9d7c-4a1f-eb60-ddecec3dc6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "E: Unable to locate package libnvvm3\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Using cached pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.2\n",
            "Collecting cupy-cuda11x\n",
            "  Downloading cupy_cuda11x-13.2.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda11x) (1.26.4)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda11x) (0.8.2)\n",
            "Downloading cupy_cuda11x-13.2.0-cp310-cp310-manylinux2014_x86_64.whl (95.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cupy-cuda11x\n",
            "Successfully installed cupy-cuda11x-13.2.0\n",
            "Looking in links: https://rapidsai.github.io/rapidsai-csp-utils/cu11\n",
            "Collecting cuml-cu11\n",
            "  Downloading cuml_cu11-24.8.0.tar.gz (2.1 kB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cupy as cp\n",
        "from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
        "from cuml.svm import SVC as cuSVC\n",
        "from cuml.naive_bayes import MultinomialNB as cuMultinomialNB\n",
        "from cuml.ensemble import RandomForestClassifier as cuRandomForestClassifier\n",
        "from cuml.neural_network import MLPClassifier as cuMLPClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Step 2: Model definition\n",
        "class CustomLogisticRegression:\n",
        "    def __init__(self):\n",
        "        self.model = cuLogisticRegression(max_iter=1000)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return self.model.predict(X_test)\n",
        "\n",
        "class CustomSVM:\n",
        "    def __init__(self):\n",
        "        self.model = cuSVC(probability=True)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return self.model.predict(X_test)\n",
        "\n",
        "class CustomNaiveBayes:\n",
        "    def __init__(self):\n",
        "        self.model = cuMultinomialNB()\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return self.model.predict(X_test)\n",
        "\n",
        "class CustomRandomForest:\n",
        "    def __init__(self):\n",
        "        self.model = cuRandomForestClassifier()\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return self.model.predict(X_test)\n",
        "\n",
        "class CustomMLP:\n",
        "    def __init__(self):\n",
        "        self.model = cuMLPClassifier(max_iter=1000)\n",
        "\n",
        "    def train(self, X_train, y_train):\n",
        "        self.model.fit(X_train, y_train)\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        return self.model.predict(X_test)\n",
        "\n",
        "# Assuming train_x and train_y are already defined as CuPy arrays\n",
        "\n",
        "# Initialize models\n",
        "log_reg = CustomLogisticRegression()\n",
        "svm = CustomSVM()\n",
        "naive_bayes = CustomNaiveBayes()\n",
        "random_forest = CustomRandomForest()\n",
        "mlp = CustomMLP()\n",
        "\n",
        "# Train models\n",
        "log_reg.train(train_x, train_y)\n",
        "svm.train(train_x, train_y)\n",
        "naive_bayes.train(train_x, train_y)\n",
        "random_forest.train(train_x, train_y)\n",
        "mlp.train(train_x, train_y)\n",
        "\n",
        "# Collect predictions (using train_x as a placeholder for validation data)\n",
        "predictions = {\n",
        "    \"log_reg\": log_reg.predict(train_x),\n",
        "    \"svm\": svm.predict(train_x),\n",
        "    \"naive_bayes\": naive_bayes.predict(train_x),\n",
        "    \"random_forest\": random_forest.predict(train_x),\n",
        "    \"mlp\": mlp.predict(train_x),\n",
        "}\n",
        "\n",
        "# Convert predictions to NumPy for AdaBoost\n",
        "stacked_features = np.column_stack([cp.asnumpy(predictions[key]) for key in predictions])\n",
        "\n",
        "# Initialize and train AdaBoost\n",
        "ada_boost = AdaBoostClassifier(n_estimators=10, random_state=42)\n",
        "ada_boost.fit(stacked_features, cp.asnumpy(train_y))\n",
        "\n",
        "# Evaluate\n",
        "stacked_accuracy = ada_boost.score(stacked_features, cp.asnumpy(train_y))\n",
        "print(\"Stacked model accuracy:\", stacked_accuracy)"
      ],
      "metadata": {
        "id": "ihWK4dogCDQH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}