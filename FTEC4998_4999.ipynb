{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzAtlDj2Mk4AopQuHkSdkO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Marcusleeleelee/FTEC4998-4999/blob/main/FTEC4998_4999.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Import the packages - ok\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from time import sleep\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9Irism69Bo6L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba489fd-d8e1-482c-9ddd-8e72b2ebfe8c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Utils - ok\n",
        "def uni_list(input): return list(set(input))\n",
        "def perform_pca(df, n_components):\n",
        "\n",
        "    # Performing PCA\n",
        "    pca = PCA(n_components=n_components)\n",
        "    principal_components = pca.fit_transform(df)\n",
        "\n",
        "    # Creating a DataFrame with the top 15 components\n",
        "    pca_df = pd.DataFrame(data=principal_components, index=df.index)\n",
        "\n",
        "    # Retaining the original column names for the new DataFrame\n",
        "    retained_columns = df.columns[:n_components]\n",
        "    pca_df.columns = retained_columns\n",
        "\n",
        "    return pca_df"
      ],
      "metadata": {
        "id": "ndu77M8kBr-h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Dataset\n",
        "class Dataset:\n",
        "    def __init__(self, file_path): # ok\n",
        "        self.dataset = pd.read_feather(file_path)\n",
        "        self.train_dict, self.test_dict = {}, {}\n",
        "        self.scalers = None\n",
        "        self.pca = None\n",
        "        self.label = 'loan_condition_cat'\n",
        "\n",
        "    def show(self, rows=10): # ok\n",
        "        return self.dataset.head(rows)\n",
        "\n",
        "    def get(self, type, key): # ok\n",
        "        if type == 'test': return self.test_dict[key]['x'], self.test_dict[key]['y']\n",
        "        elif type == 'train': return self.train_dict[key]['x'], self.train_dict[key]['y']\n",
        "        else: raise Exception('The type must be either \"test\" or \"train\"!')\n",
        "\n",
        "    def update(self, type, key, x, y): # ok\n",
        "        if type == 'test': self.test_dict.update({key: {'x': x, 'y': y}})\n",
        "        elif type == 'train': self.train_dict.update({key: {'x': x, 'y': y}})\n",
        "        else: raise Exception('The type must be either \"test\" or \"train\"!')\n",
        "\n",
        "    def basic_processing(self): # ok\n",
        "        temp_func_1 = lambda x: '<=2009' if str(x) in ['2007', '2008', '2009'] else (\"[2010, 2012]\" if str(x) in ['2010', '2011', '2012'] else '>=2013')\n",
        "        columns_to_delete = [\n",
        "            'id', 'issue_d', 'home_ownership_cat', 'income_category', 'income_cat', 'term_cat', 'application_type_cat',\n",
        "            'purpose_cat', 'interest_payment_cat', 'loan_condition'\n",
        "        ]\n",
        "        self.dataset.drop(columns=columns_to_delete, inplace=True)\n",
        "        self.dataset['grade'] = self.dataset['grade'].apply(temp_func_1)\n",
        "        self.dataset['final_d'] = self.dataset['final_d'].apply(lambda x: str(x)[-4:]).apply(temp_func_1)\n",
        "        self.dataset = pd.get_dummies(self.dataset, columns=['year', 'final_d', 'home_ownership', 'term', 'application_type',\n",
        "                                                             'purpose', 'interest_payments', 'grade', 'region'], dtype=int)\n",
        "\n",
        "    def train_test_split(self, percentage=0.8): # ok\n",
        "        self.dataset = self.dataset.sample(frac=1).reset_index(drop=True)\n",
        "        train_size = int(len(self.dataset) * percentage)\n",
        "        temp1 = self.dataset.iloc[:train_size].copy()\n",
        "        temp2 = self.dataset.iloc[train_size:].copy()\n",
        "\n",
        "        y_train = temp1[[self.label]]\n",
        "        x_train = temp1.drop(columns=[self.label])\n",
        "        self.update('train', 'w0', x_train, y_train)\n",
        "\n",
        "        y_test = temp2[[self.label]]\n",
        "        x_test = temp2.drop(columns=[self.label])\n",
        "        self.update('test', 'w0', x_test, y_test)\n",
        "\n",
        "    def preprocessing_train(self): # ok\n",
        "        temp_train_x, temp_train_y = self.get('train', 'w0')\n",
        "        scaler = StandardScaler()\n",
        "        temp_train_x = pd.DataFrame(scaler.fit_transform(temp_train_x), columns=temp_train_x.columns)\n",
        "        self.scalers = scaler\n",
        "        temp_train_x = perform_pca(temp_train_x, n_components=30)\n",
        "        self.update('train', 'w0', temp_train_x, temp_train_y)\n",
        "\n",
        "    def preprocessing_test(self): # Not ok\n",
        "        temp_test_x, temp_test_y = self.get('test', 'w0')\n",
        "        temp_train_x, _ = self.get('train', 'w0')\n",
        "        print(temp_train_x)\n",
        "\n",
        "        # Apply stored scalers\n",
        "        temp_test_x = pd.DataFrame(self.scalers.transform(temp_test_x), columns=temp_train_x.columns)\n",
        "\n",
        "        assert set(temp_test_x.columns) == set(temp_train_x.columns)\n",
        "        self.update('test', 'w0', temp_test_x, temp_test_y)"
      ],
      "metadata": {
        "id": "lmLx5bbgBtcq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating # ok\n",
        "data = Dataset('/content/drive/My Drive/Colab Notebooks/FTEC4998_9/loan_final313_processed.feather')\n",
        "data.basic_processing()\n",
        "data.train_test_split()\n",
        "data.preprocessing_train()"
      ],
      "metadata": {
        "id": "13kLSUwPBvO9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data conversion # ok\n",
        "train_x, train_y = data.get('train', 'w0')\n",
        "X_train, y_train = train_x.to_numpy(), train_y.values.ravel()\n",
        "counts = np.mean(y_train == 1) * 100\n",
        "print(counts)\n",
        "print(X_train.shape, y_train.shape)"
      ],
      "metadata": {
        "id": "EMPCAhUjB4tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a91f9e49-60de-4168-b971-216f5dabe6c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.587092884520843\n",
            "(709903, 30) (709903,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Model training\n",
        "# Ensure y_train is binary\n",
        "assert set(y_train).issubset({0, 1}), \"Target values must be 0 or 1 for binary classification.\"\n",
        "\n",
        "# Convert to tensors and move to GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "# Convert y_train to -1 and 1 for SVM\n",
        "y_train_svm = y_train * 2 - 1\n",
        "y_train_svm_tensor = torch.tensor(y_train_svm, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "# Logistic Regression as a neural network\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, X_train_tensor, y_train_tensor, lr=0.001):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.input_dim = X_train_tensor.shape[1]\n",
        "        self.linear = nn.Linear(self.input_dim, 1)\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=lr)\n",
        "        self.epochs = 100\n",
        "        self.y_train_tensor = y_train_tensor\n",
        "        self.X_train_tensor = X_train_tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "# MLP model\n",
        "class ANN(nn.Module):\n",
        "    def __init__(self, X_train_tensor, y_train_tensor, lr=0.001):\n",
        "        super(ANN, self).__init__()\n",
        "        self.input_dim = X_train_tensor.shape[1]\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(self.input_dim, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(64, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        self.criterion = nn.BCELoss()\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
        "        self.epochs = 100\n",
        "        self.y_train_tensor = y_train_tensor\n",
        "        self.X_train_tensor = X_train_tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# SVM Model\n",
        "class SVM(nn.Module):\n",
        "    def __init__(self, X_train_tensor, y_train_tensor, lr=0.001):\n",
        "        super(SVM, self).__init__()\n",
        "        self.input_dim = X_train_tensor.shape[1]\n",
        "        self.linear = nn.Linear(self.input_dim, 1)\n",
        "        self.criterion = nn.HingeEmbeddingLoss()\n",
        "        self.optimizer = optim.AdamW(self.parameters(), lr=lr)\n",
        "        self.epochs = 100\n",
        "        self.y_train_tensor = y_train_tensor\n",
        "        self.X_train_tensor = X_train_tensor\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Train and predict functions\n",
        "def train_model(model):\n",
        "    model.train()\n",
        "    for epoch in range(model.epochs):\n",
        "        model.optimizer.zero_grad()\n",
        "        outputs = model(model.X_train_tensor)\n",
        "        loss = model.criterion(outputs, model.y_train_tensor)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "\n",
        "def predict_model_train(model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(model.X_train_tensor).squeeze()\n",
        "    return (outputs > 0.5).float()\n",
        "\n",
        "# Initialize PyTorch models and move to GPU\n",
        "log_reg_model = LogisticRegressionModel(X_train_tensor, y_train_tensor).to(device)\n",
        "ann_model = ANN(X_train_tensor, y_train_tensor).to(device)\n",
        "svm_model = SVM(X_train_tensor, y_train_svm_tensor).to(device)\n",
        "print(log_reg_model.criterion)\n",
        "print(ann_model.criterion)\n",
        "print(svm_model.criterion)"
      ],
      "metadata": {
        "id": "KQxTOBOkHlsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f0280a-be44-4a7b-9df7-b8d2028e3faf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BCELoss()\n",
            "BCELoss()\n",
            "HingeEmbeddingLoss()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train PyTorch models\n",
        "train_model(log_reg_model)\n",
        "train_model(ann_model)\n",
        "train_model(svm_model)"
      ],
      "metadata": {
        "id": "NadN225Ro4F2"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict with PyTorch models and move predictions to CPU\n",
        "log_reg_predictions = predict_model_train(log_reg_model).cpu().numpy()\n",
        "ann_predictions = predict_model_train(ann_model).cpu().numpy()\n",
        "svm_predictions = predict_model_train(svm_model).cpu().numpy()"
      ],
      "metadata": {
        "id": "7xY6uPCbr9ru"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect predictions for stacking\n",
        "predictions = {\n",
        "    \"log_reg\": log_reg_predictions,\n",
        "    \"ANN\": ann_predictions,\n",
        "    'svm':svm_predictions\n",
        "}"
      ],
      "metadata": {
        "id": "ypQUg_R6samW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stack predictions as features for AdaBoost\n",
        "stacked_features = np.column_stack(list(predictions.values()))\n",
        "\n",
        "# Initialize and train AdaBoost\n",
        "ada_boost = AdaBoostClassifier(n_estimators=30, random_state=42)\n",
        "ada_boost.fit(stacked_features, y_train)\n",
        "\n",
        "# Evaluate train\n",
        "stacked_accuracy = ada_boost.score(stacked_features, y_train)\n",
        "print(\"Stacked model accuracy:\", stacked_accuracy)"
      ],
      "metadata": {
        "id": "9YtaBSy9ZyUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db6c6df4-767e-412e-c7be-1322e0bb03a9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stacked model accuracy: 0.9382070508224363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fkw5jbXSIpfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PK_0JVF6IZu-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}